<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Self-training/Pseudo Label Methods</title>
    <link href="/2021/06/10/Self-training-Pseudo-Label-Methods/"/>
    <url>/2021/06/10/Self-training-Pseudo-Label-Methods/</url>
    
    <content type="html"><![CDATA[<h2 id="Self-training-伪标签-方法"><a href="#Self-training-伪标签-方法" class="headerlink" title="Self-training / 伪标签 方法"></a>Self-training / 伪标签 方法</h2><font face='楷体'> 近期对半监督学习中自训练（伪标签）算法进行了了一些调研，改方法的通用框架就是用有标签数据训练教师模型后，生成未标注数据供学生模型训练，改方法有效的一种解释是，能够使得模型的决策边界更平滑，也就是基于半监督学习中的**平滑假设**。该算法虽然能为模型带来一定的提升，但由于性能有限，近年来半监督学习的研究热点多凝聚于一致性正则化(Consistency regularization)上，但近期也有些方法依旧是对self-training（伪标签）技术的扩展，通过引入课程学习、元学习等方法，来进一步提高self-training的性能，本文对这些内容进行总结。 </font><h3 id="经典的self-training算法"><a href="#经典的self-training算法" class="headerlink" title="经典的self-training算法"></a>经典的self-training算法</h3><p><img src="https://pic1.zhimg.com/80/v2-84d778db8dd4912c86a15c4782fd4020_1440w.jpg" alt="img"></p><p>如上图所示，Self-training 的训练过程如下：</p><p><strong>Step1：</strong>首先，用少量的标签数据 L 训练 Model；也就是上图的虚线以上部分；</p><p><strong>Step2：</strong>然后，使用训练后的 Model 给未标记的数据点 x∈U 分配 Pseudo-label（伪标签）；</p><p><strong>Setp3：</strong>通过交叉熵损失计算模型预测和伪标签的损失。</p><p><strong>Step4：</strong>最后，使用训练好的模型为 U 的其余部分生成代理标签，一直循环，直到模型无法生成代理标签为止。</p><p>以下就是 Self-training 的伪代码：</p><p><img src="https://pic3.zhimg.com/80/v2-2317e8030f54e859792962dfc2c2de1a_1440w.jpg" alt="img"></p><p>而 Pseudo-label 与 Self-training 基本思想是一致的，但这类方法主要缺点是：模型无法纠正自己的错误。如果模型对自己预测的结果很有“自信”，但这种自信是盲目的，那么结果就是错的，这种偏差就会在训练中得到放大。</p><h3 id="Curriculum-Labeling-Revisiting-Pseudo-Labeling-for-Semi-Supervised-Learning"><a href="#Curriculum-Labeling-Revisiting-Pseudo-Labeling-for-Semi-Supervised-Learning" class="headerlink" title="Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning"></a>Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning</h3><p>链接：<a href="https://arxiv.org/pdf/2001.06001.pdf">https://arxiv.org/pdf/2001.06001.pdf</a></p><p>来源：AAAI 2021</p><h5 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h5><p>这篇文章在半监督学习的背景下重新讨论伪标签的思想。伪标签的工作原理是在无标签的样本集中使用伪标签，通过伪标签和已有的标签数据训练模型，通过self-training的循环重复迭代这个过程。现有的方法似乎已经抛弃了这种方式，而倾向于一致性正则化方法，这种方法结合不同风格的自监督损失对无标签样本以及监督损失对有标签样本进行训练。经验证明，伪标签实际上可以与最好的方法相竞争，同时在无标签样本集中，对于ood样本更具弹性。文中通过两个关键因素来使伪标签取得这样的显著的结果：1）应用课程学习原则；2）在每次自训练循环前通过重新启动参数来防止概念漂移；<code>确认偏差与概念漂移现象有关，即目标变量的属性会随着时间的推移而改变。在伪标记中，这一点值得特别注意，因为目标变量受到同一模型的影响</code></p><h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>文中的方法来源于课程学习中先学习简单样本在逐步提升学习困难样本，因此文中的主要挑战是涉及一个课程——如何控制学习步调。文中设计了一种 self-pace 的策略，通过分析在无标签样本上的预测分数的分布以及应用基于极限值理论（EVT）的准则。模型如下：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20210206201738.png" alt="image-20210206201736890"></p><p>最近，Oliver et al (Oliver et al. 2018)强调了SSL方法评估中的一个当前问题，该问题批评了将一小部分训练数据作为“有标签的”，将大量相同的训练数据作为“无标签的”的普遍做法。这种数据分割的方式会导致这样一种场景:<strong>在这种类型的“未标记”集合中的所有图像都隐式地保证它们来自与“已标记”集合完全相同的类分布</strong>。</p><h5 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h5><p>在半监督学习中，数据集 $D=\{x|x\in X\}$ 由有标签子集 $D_L=\{(x,y)|x \in X,y\in Y\}$ 和无标签子集 $D_{UL}=\{x|x \in X\}$ 构成，其中 x，y 表示输入和对应的标签，通常 $|D_L| \ll |D _ {UL}|$；</p><p>伪标签建立在通常的 self-training 框架上，模型首先在 DLDL 上进行训练，后续的训练则是在 DLDL 以及伪标注的无标签子集 DULDUL 上。在 tt 轮次，令训练样本表示为 $(X_t,Y_t)$，当前模型表示为 $P _ {\theta}^t$，其中 θ 为模型参数， 且 t∈{1,⋯,T}；在 t 轮后，无标签子集 $\bar{X_t}$ 被并入 $X _ {t+1}:=X_1\cup \bar{X_t}$，并且新的目标集定义为 $Y _ {t+1}:=Y_1\cup \bar{Y_t}$，这里 $\bar{Y_t}$ 代表当前模型 $P _ {\theta}^t$ 为 $\bar{X_t}$ 打上的伪标签；<code>这些都是常规的 self-traiing步骤</code></p><p>本文的关键是用于<strong>确定在每个训练轮次中将无标签子集中哪些样本融入到训练样本</strong>的准则。已有的方法包括选择具有高置信度的样本，或在特征空间中检索最近的样本。文中从极值理论（Extreme Value Theory: 用来模拟一维概率分布尾部的极端事件）中汲取了一些观点。在本文的问题中，作者观察到伪标签数据的最大概率预测分布遵循这种类型的 Pareto 分布。因此，我们不是使用固定的阈值或手动调整阈值，而是使用百分位分数来决定添加哪些样本。下图中的算法描述了本文模型的算法流程；</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20210206215sda504.png" alt="image-20210206215503038"></p><p>其中 Percentile(X,TR)Percentile(X,TR) 返回第 r 个百分位数；</p><blockquote><p>关于算法，我的理解是，首先设定一个阈值，利用百分位数函数 Percentile() 获取当前轮次中高于该阈值的预测结果并将其加入有标签集，重新训练一次模型，之后逐渐降低阈值；感觉上创新点并不是很多，另外关于极值理论在本算法中的应用也不是很理解，原文中还有一节理论分析，或许有答案；</p></blockquote><h3 id="Meta-Pseudo-Labels"><a href="#Meta-Pseudo-Labels" class="headerlink" title="Meta Pseudo Labels"></a>Meta Pseudo Labels</h3><p>Link: <a href="https://arxiv.org/pdf/2003.10580.pdf">https://arxiv.org/pdf/2003.10580.pdf</a></p><p>来源：Google AI Brain</p><h5 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h5><p>文中提出了一种元伪标签模型，包括一个教师网络来生成伪标签用于指导学生网络，不同于伪标签算法中教师网络参数固定，在本文中教师网络的参数也随着学生网络在有标签数据集上表现的反馈而进行调整；</p><h5 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h5><p>伪标签方法中，如果教师模型标注的伪标签是不正确的，那么学生模型从不正确的数据中学习，其性能很难超过教师模型，这种反馈被称为伪标注的确认偏差。本文设计了一种系统化机制，让教师模型通过观察其伪标签如何影响到学生模型，以此纠正偏差。该方法称为 Meta Pseudo Labels，利用学生模型的反馈来提醒教师模型产生更好的伪标签。即教师模型和学生模型的训练是并行的：1）学生模型学习教师模型标注的伪标签数据；2）教师模型从学生模型在有标签数据集上的表现所反馈的奖励种进行学习；</p><h5 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h5><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20210209163400.png" alt="image-20210209163359148"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20210209163430.png" alt="image-20210209163428879"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/202102091625s10.png" alt="image-20210209162448194"></p><p>不同与伪标签算法，本文的想法是通过优化学生模型在有标签数据上的目标函数 LlLl，来对教师模型参数 θTθT 的优化提供信号，但是要计算相关梯度非常复杂，文中提供了一种元学习中的近似方法：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20210209163348.png" alt="image-20210209163346810"></p><p>将上式代入式(2) 中，得到元伪标签模型的教师模型优化目标：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20210209163540.png" alt="image-20210209163537256"></p><p>教师模型和学生模型的优化：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20210209164258.png" alt="image-20210209164254577"></p><p>此外，文中还发现为教师模型添加一些辅助损失能提升性能；完整的算法流程如下：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20210209164511.png" alt="image-20210209164509605"></p><blockquote><p>整体上可以这么理解文中算法：首先是有一批有标签数据和无标签数据，以及初始化的教师模型和学生模型，用教师模型在无标签数据集上生成伪标签，然后用得到的伪标签数据训练学生模型并更新参数，用更新后的学生模型在有标签数据集上进行训练计算loss，用该loss优化教师模型的参数，重复下去；此外，作者发现可以用有标签数据微调学生模型能带来提升，为教师模型添加一些自监督辅助任务也能有提升；</p></blockquote><h3 id="Self-training-with-Noisy-Student-improves-ImageNet-classification（CVPR2020）"><a href="#Self-training-with-Noisy-Student-improves-ImageNet-classification（CVPR2020）" class="headerlink" title="Self-training with Noisy Student improves ImageNet classification（CVPR2020）"></a>Self-training with Noisy Student improves ImageNet classification（CVPR2020）</h3><p><a href="https://arxiv.org/abs/1911.04252">link</a></p><h5 id="What-is-Noisy-Student"><a href="#What-is-Noisy-Student" class="headerlink" title="What is Noisy Student?"></a>What is Noisy Student?</h5><p>Self-training是利用未标记数据的好方法。但这篇来自Google的文章却强调了Noisy Student。怎么回事？它与经典方法有什么不同吗？</p><p>Noisy Student的作者发现，要使这种方法发挥作用，student model在训练过程中应加噪声，如dropout, stochastic depth andaugmentation等。而teacher model在产生伪标签时不应加噪声。因为Noisy 是整个算法的一个重要部分，所以他们称之为“Noisy Student”。</p><p>Noisy Student与经典的自学习算法相似，但主要区别在于使用不同的方法给学生增加噪音。需要注意的是，当生成伪标签时，教师并没有被噪声干扰。</p><p><img src="https://pic1.zhimg.com/80/v2-241ae2ad68eb85b0896e48a0f5c5ef50_1440w.jpg" alt="img">img</p><p>除了噪音，还有两点对Noisy Student来说很重要。</p><p>（1）尽管教师和学生的结构可以相同，但学生模型的容量应该更高。为什么？因为它必须适合一个更大的数据集（标签和伪标签）。因此，学生模型必须大于教师模型。</p><p>（2）平衡数据：作者发现，当每个类的未标记图像数量相同时，学生模型效果良好。</p><font face="楷体"> 观察上述三篇近一年内的文章，分别是将self-training与课程学习、元优化策略、噪声鲁棒性等方法结合：其中第一篇认为伪标签的学习应该根据课程学习的原则，由易渐难，并且处理了概念漂移问题，可以认为篇文章是对self-training过程中无标签样本的采样进行改进；第二篇的想法更直接——既然我们的目标是提升学生模型在 教师模型标注的伪标签数据 上训练后的效果，那我们的就应该让学生模型训练的结果反馈到教师模型的参数更新上，然而这种方法难以计算梯度，遂借鉴了元学习MAML中的优化策略，近似计算梯度，使得上述方法能够得以实现，此外，作者还在教师模型的训练过程中加入一些自监督任务（相当于一致性正则化的思想），也能提升模型效果，因此，这篇文章可以看作是对self-training算法中教师-学生模型交互的改进，使两者能够在训练中耦合在一起；第三篇文章的思想则是改进学生模型，使其比教师模型更大且用带大量noise的数据训练，增强鲁棒性，算法简单粗暴，但是有效，并且有一篇EMNLP2020上的工作将其引入到disfluency detection的工作上，可以将这种算法看作对self-training的数据增强;由此可见，self-training中其实由很多可挖的点，与一些新技术的结合，即使是一些很细微的改进也能带来性能的提升，因此self-training技术其实并未过时，这种简单有效的方法，只要加上一些其它约束，就能够得到进一步的性能提升。</font><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>【1】长文总结半监督学习（Semi-Supervised Learning） - PaperWeekly的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/252343352">https://zhuanlan.zhihu.com/p/252343352</a></p><p>【2】半监督学习 - 马东什么的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/349107869">https://zhuanlan.zhihu.com/p/349107869</a></p><p>【3】Self-training with Noisy Student - yanwan的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/164597142">https://zhuanlan.zhihu.com/p/164597142</a></p><h3 id="补充：IN-DEFENSE-OF-PSEUDO-LABELING-AN-UNCERTAINTY-AWARE-PSEUDO-LABEL-SELECTION-FRAMEWORK-FOR-SEMI-SUPERVISED-LEARNING"><a href="#补充：IN-DEFENSE-OF-PSEUDO-LABELING-AN-UNCERTAINTY-AWARE-PSEUDO-LABEL-SELECTION-FRAMEWORK-FOR-SEMI-SUPERVISED-LEARNING" class="headerlink" title="补充：IN DEFENSE OF PSEUDO-LABELING: AN UNCERTAINTY-AWARE PSEUDO-LABEL SELECTION FRAMEWORK FOR SEMI-SUPERVISED LEARNING"></a>补充：IN DEFENSE OF PSEUDO-LABELING: AN UNCERTAINTY-AWARE PSEUDO-LABEL SELECTION FRAMEWORK FOR SEMI-SUPERVISED LEARNING</h3><p>ICLR 2021</p><p><a href="https://arxiv.org/pdf/2101.06329.pdf">https://arxiv.org/pdf/2101.06329.pdf</a></p><p>伪标签还能这样用？半监督力作UPS（ICLR 21）大揭秘！ - 罗驳思的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/350701042">https://zhuanlan.zhihu.com/p/350701042</a></p><p><strong>不确定性估计与伪标签技术的结合</strong></p><h5 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h5><p>最近在半监督学习（SSL）中的研究主要由基于一致性正则化的方法主导，这些方法可实现出色的性能。但是，它们严重依赖于特定域的数据扩充，这对于所有数据模态来说都不容易生成。伪标签（PL）是一种通用的SSL方法，它没有此限制，但在其原始形式中效果相对较差。作者认为，伪标签（PL）的表现不佳，是由于模型校准不正确造成的高置信度预测错误；这些预测会产生许多错误的伪标签，从而导致训练繁琐。作者提出了一种不确定性感知的伪标签选择（UPS）框架，该框架通过大幅度减少训练过程中遇到的噪声量来提高伪标签的准确性。此外，UPS推广了伪标签过程，从而可以创建负样本伪标签。这些负样本伪标签可用于多标签分类以及用于改进单标签分类的负样本学习。与CIFAR-10和CIFAR-100数据集上的最新SSL方法相比，文章获得了出色的性能。此外，作者还表示该方法在视频数据集UCF-101和多标签数据集Pascal VOC上也取得了不错的性能。</p><h5 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h5><p>SSL的一个常见的假设是决策边界应该位于低密度区域，基于一致性-正则化的方法通过令网络的输出在小的输入扰动下保持不变来实现这一点，然而，这些方法的一个问题使，它们依赖于数据增强得到的丰富扩展集，例如仿射转换、裁剪等，这限制了它们在增强效果较差的领域的能力。基于伪标签的方法选择置信度搞的无标签样本为训练目标（伪标签），这可以看作是熵最小化的一种形式，它减少了决策边界上的数据点密度。与一致性正则化相比，伪标记的一个优点是它本质上不需要扩充，并且可以广泛应用于大多数领域。然而，最近的一致性正则化方法在SSL基准测试上的表现往往优于伪标记。这项工作是为伪标记辩护的:我们证明了基于伪标记的方法与一致性正则化方法的性能相当。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Meta Label Correction for Noisy Label Learning</title>
    <link href="/2021/06/10/Meta-Label-Correction-for-Noisy-Label-Learning/"/>
    <url>/2021/06/10/Meta-Label-Correction-for-Noisy-Label-Learning/</url>
    
    <content type="html"><![CDATA[<h2 id="Meta-Label-Correction-for-Noisy-Label-Learning"><a href="#Meta-Label-Correction-for-Noisy-Label-Learning" class="headerlink" title="Meta Label Correction for Noisy Label Learning"></a>Meta Label Correction for Noisy Label Learning</h2><p>链接：<a href="https://kami.app/GHf2zo20XclM">https://kami.app/GHf2zo20XclM</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>利用弱监督或有噪监督来构建有效的机器学习模型一直是一个重要的研究问题，并且由于对用于训练模型的大规模数据集的需求越来越大，它的重要性进一步增加。弱监督或有噪监督可能有多种来源，包括非专业的标注者、基于启发式或用户交互信号的自动标注方式。有大量的前期工作集中于利用有噪的标签。最值得注意的是，最近的工作通过使用元学习实例加权的方法得到了不错的效果，在这种方法中，元学习框架用于为有噪标签分配实例权重。在本文中，作者将此方法扩展为元学习框架内的标签校正问题，将标签校正过程看作是一个元过程，并提出了一个新的元学习框架<strong>MLC (Meta Label Correction)</strong> 用于有噪标签学习。具体而言，采用标签校正网络作为元模型，对有噪标签进行校正，同时利用校正后的标签对主模型进行训练。两个模型通过求解一个双层优化问题来联合训练。在图像识别和文本分类任务中，作者对不同的标签噪声水平进行了广泛的实验，对比了重加权和修正的方法，结果表明修正框架解决了重加权的一些限制。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>有噪数据有多种来源：错误标签、非专业标注者、基于启发式或用户交互信号的自动标注等。通过有噪标签训练深度神经网络是很具挑战性的，因为深度网络很容易你和并记住噪声，因此为了更有效地进行学习，最近提出了多种方法来有效地结合干净标签的数据和有噪标签的数据。其中一项工作是侧重于通过 co-teaching 或课程学习方法从有噪数据中选择那些可能是正确标注的样本；另一类工作则试图为弱监督实例去重新赋权重用于选择性训练，而不是包括或者排除它们。有些方法使用元学习框架为每个样本赋予重要性分数，使得更重要的样本能对主模型训练产生更大的贡献。</p><p>Label re-weighting 方法的一个限制就是它仅限于提高体感或者降低一个样本在学习过程中的贡献，另一种方法则是标签校正。它的目的是在对弱标签生成过程进行一定假设的基础上对有噪声的标签进行修正，某种意义上说，标签校正的目的不仅仅是选择或赋予有用的示例较高的权重，还包括改变标签错误的示例的标签。然而之前的标签校正方法依赖于对弱标签生成过程的交涉，因此往往涉及两个独立的步骤：(1) 估计一个标签损坏矩阵；(2) 利用损坏矩阵训练一个基于噪声数据的模型；估计损坏矩阵通常涉及到关于噪声产生过程的假设，例如，假设有噪标签只依赖于真实的标签，并且独立于数据本身。</p><p>这篇文章中，作者从元学习的角度，采用标签校正法来解决有噪标签的学习问题，作者将该方法称为元标签校正 (<strong>MLC</strong>)。具体来说，就是把标签校正过程看作一个元过程，这意味着它的目标是为弱标签的样本提供正确的标签。同时，利用元模型生成的校正后的表亲啊对主模型进行训练，元模型和主模型通过双层优化过程并行学习，通过一种可微的方式更新标签校正过程使模型在干净数据集（干净标签作为验证集）上的性能最大化。<strong>MLC</strong> 扩展了利用这两种方法的优点，与基于实例权重重新赋值的元学习方法不同的是，<strong>MLC</strong> 提供了一个更精确的方法利用有噪标签，它能够探索标签空间中所有可能的类；与之前的标签校正方法不同的是，<strong>MLC</strong> 没有对潜在的标签噪声做假设，并且同时学习校正模型和主模型。下图示例了re-weighting方法最多可以降低有噪样本的权值，减少它们对学习过程的影响，而 <strong>MLC</strong> 可以成功地将有噪声的标签校正为真实的标签：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20201213213533.png" alt="image-20201213213532316" style="zoom:50%;" /></p><p><strong>Contributions</strong></p><ul><li>文中将有噪监督学习任务看作一个元标签校正过程，其中校正网络在元过程阶段训练来为主模型的训练提供可靠的标签；</li><li>文中对比了 re-weighting 方法和校正方法在处理有噪标签数据的两种策略；</li><li>作者在3个图像识别和4个大规模文本分类任务上进行了实验，这些任务具有不同的噪声水平和类型，包括真实世界的噪声标签。实验结果表明，该方法在标签校正和权重调整方面由于之前最好的方法，证明了该方法的有效性。</li></ul><h4 id="Learning-with-label-correction"><a href="#Learning-with-label-correction" class="headerlink" title="Learning with label correction"></a>Learning with label correction</h4><p>第一类工作的目的是通过假设噪声标签的产生方式，尽可能地纠正弱标签。考虑一个问题，将数据分为 k 类，标签纠正过程包括估计一个标签损害矩阵 $C_{k\times k}$, 其中 $C_{ij}$ 表示观测标签为 $i$, 而真实标签为 $j$ 的概率；这种方式的缺点是，标签损害矩阵的估计是一种特定的方式，而且估计过程是独立于主模型过程，因此不允许从主模型反馈到估计过程。</p><h4 id="Learning-to-re-weight-training-instances"><a href="#Learning-to-re-weight-training-instances" class="headerlink" title="Learning to re-weight training instances"></a>Learning to re-weight training instances</h4><p>另一类工作则侧重于从噪声数据中分离出可能是正确样本的子集，这种方法的一个扩展是为每个样本分配一个可学习的权重。实例的权重本质上是主模型的超参数，可以通过构造一个双层优化问题来学习。这个框架允许示例权重学习和主模型相互交流，从而可以学习到更好的模型。</p><p>文中的方法是通过在有噪样本中学习建模和校正标签噪声，不同于分别处理标签校正和模型学习，该方法是通过元学习来协同优化这两个步骤。</p><h4 id="Meta-Label-Correction"><a href="#Meta-Label-Correction" class="headerlink" title="Meta Label Correction"></a>Meta Label Correction</h4><p>根据之前一些工作的设定，作者先假设弱监督下的学习设置包含两组数据：一组是带有干净标签的数据，另一组为有噪标签；通常，前者比后者小得多。直接在小的干净集上训练往往是次优的，因为太少的数据很容易导致过拟合。直接在有噪声的集合(或有噪声和干净集合的组合)上进行训练也往往不是最优的，因为大容量的模型可以拟合和记忆噪声。标签校正方法的一个优点是，它允许我们在学习过程中结合干净的标签和校正过的标签。作者提出的方法采用了标签校正方法，同时通过统一的元学习框架与主模型过程共同优化标签校正过程。作者通过训练一个元学习者(元模型)来纠正嘈杂的标签，以及一个主模型，用来自元模型的纠正过的标签来构建最佳的预测模型，从而使元模型和主模型能够相互加强。</p><h5 id="A-meta-learning-based-formulation-for-label-correction"><a href="#A-meta-learning-based-formulation-for-label-correction" class="headerlink" title="A meta-learning based formulation for label correction"></a>A meta-learning based formulation for label correction</h5><p>给定一个干净数据集 $D=\{x,y\}^m$, 和一个有噪数据集 $D^\prime =\{x,y^\prime\}^M$, m远小于M；为了更好地利用弱标签中携带的信息，作者提出构造一个标签校正网络 <strong>LCN</strong>，作为元模型，以一对有噪声的数据样本和弱标签作为输入，并尝试为该数据生成一个校正标签。<strong>LCN</strong> 是一个参数为 $\alpha$ 的函数，$y_c=g_\alpha (h(x),y^\prime)$ 纠正 x 的原始有噪标签 $y^\prime$，得到的 $y_c$ 是一个soft标签；与此同时，主模型 $f$ 实例化为另一个函数 $y=f_w(x)$. </p><p>不连接这两个模型，就：1）没办法保证从LCN得到的校正后的标签是真正有意义的，因为没有干净的标签直接就训练LCN是不可能的；2）如果LCN提供的标签与未知的真实标签不一致，如何保证主模型的输出能够拟合到正确的标签上；幸运的是，这两个模型通过双层优化框架能够被链接到一起，其动机是基于这样一种直觉：如果LCN生成的标签质量高，那么使用这些经过校正的标签进行监督训练的分类器在单独一组干净示例上的损失就会小。形式上可以用下式表示：</p><script type="math/tex; mode=display">\min _\alpha \mathbb{E}_{(x,y)\in D}l(y,f_{w_\alpha^*}(x))\\s.t.\ w_\alpha^* =arg\min_w \mathbb{E}_{(x,y')\in D'}l(g_\alpha(h(x),y'),f_w(x))</script><p>文中将此框架称为 元标签校正(<strong>MLC</strong>)，下图为框架的概览：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20201213213444.png" alt="image-20201213213433150"></p><p>在双层优化中，LCN 的参数 $\alpha$ 是上层参数（元参数），而主模型参数 $w$ 是底层参数（主参数）；</p><h5 id="Efficient-meta-gradient-with-k-step-SGD-from-main-parameters"><a href="#Efficient-meta-gradient-with-k-step-SGD-from-main-parameters" class="headerlink" title="Efficient meta-gradient with k-step SGD from main-parameters"></a>Efficient meta-gradient with k-step SGD from main-parameters</h5><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/202012dsada13224730.png" alt="image-20201213224440574" style="zoom: 50%;" /></p><p>上图为不同元优化次数的示意图；</p><h5 id="Training-with-soft-labels-from-LCN"><a href="#Training-with-soft-labels-from-LCN" class="headerlink" title="Training with soft labels from LCN"></a>Training with soft labels from LCN</h5><p>LCN 不仅明确地对校正标签在数据样本和它的有噪标签上的依赖性进行建模，而且它还确保 LCN 的输出是对所有可能的类的有效分类分布。在MLC中，软标签是至关重要的，因为它们使梯度从主模型返回到元模型成为可能。但是，当主模型采用这些带有软修正标签的例子时，由于修正标签中附加的不确定性，给训练带来了困难。这可以通过以下策略来缓解。在训练每个 batch 的干净数据是，作者将其分为两部分，一部分作为干净的评估集，另一部分加入到主模型 $f$ 的训练过程中，因为干净的数据集中的一小部分能够为训练提供干净的知道，能方便模型训练。这已经被证明在类似情况下是有效的。</p><h5 id="Remark-Label-correction-vs-label-reweighting"><a href="#Remark-Label-correction-vs-label-reweighting" class="headerlink" title="Remark: Label correction vs label reweighting"></a>Remark: Label correction vs label reweighting</h5><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20201214091552.png" alt="image-20201214091549568" style="zoom:33%;" /></p><p>对比权重网络 Meta-WN 和 元标签校正 MLC：</p><ul><li>Meta-WN 只是试图为给定的有噪标签类赋值新的权重；而MLC则试图校正给定的弱标签，并考虑了所有类别的可能性；</li><li>另一个关键区别就是，WN以loss为输入，当不同数据对的loss相同时，模型就无区分能力；而MLC则不是；</li></ul><h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p><strong>Noisy Label sources</strong>   保留一千个样本作为 clean set，有噪数据集通过以下两种设置对原数据标签进行破坏而产生的：</p><ul><li><strong>Uniform label noise (UNIF)</strong> 对于一个干净样本，以概率$\frac{\rho}{C}$ 随机将其扰动到任意类别，并以 $\rho$ 的概率保持原标签；</li><li><strong>Flipped label noise (FLIP)</strong> 对于一个干净样本，以概率 $\rho$ 随机扰动到其它C-1类；</li><li><strong>Real-world noisy labels</strong> Clothing 1M数据集包含来自真实世界中的噪声；</li></ul><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20201214110545.png" alt="image-20201214110543826" style="zoom: 67%;" /></p><h5 id="Analysis-and-ablation-studies"><a href="#Analysis-and-ablation-studies" class="headerlink" title="Analysis and ablation studies"></a>Analysis and ablation studies</h5><p>作者进行了消融实验，对比分析了 元学习过程中 K 的大小对实验结果的影响，以及噪声等级对不同模型实验结果的影响；</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20201214110652.png" alt="image-20201214110650527" style="zoom:67%;" /></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/20201214124309.png" alt="image-20201214124307588" style="zoom: 33%;" /></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>句子去偏</title>
    <link href="/2021/06/10/%E5%8F%A5%E5%AD%90%E5%8E%BB%E5%81%8F/"/>
    <url>/2021/06/10/%E5%8F%A5%E5%AD%90%E5%8E%BB%E5%81%8F/</url>
    
    <content type="html"><![CDATA[<h3 id="Towards-Debiasing-Sentence-Representations"><a href="#Towards-Debiasing-Sentence-Representations" class="headerlink" title="Towards Debiasing Sentence Representations"></a>Towards Debiasing Sentence Representations</h3><p>文章链接：<a href="https://arxiv.org/abs/2007.08100">https://arxiv.org/abs/2007.08100</a> ，来源 ACL2020</p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>先前的研究已经揭示出在广泛使用的词嵌入中存在社会偏见，包括性别、种族、宗教以及其它社会结构。虽然以及有一些方法被提出用于消除词级别的偏见，但考虑到最近趋向于使用使用上下文的句子表示（例如ElMo、BERT等），因此有必要对句子级的表示进行去偏。本文捉着研究了句子级表示中社会偏见的存在，并提出了一种减少偏见的方法——SENT-DEBIAS。实验表明，SENT-DEBIAS 在保留了句子级下游任务（比如情感分类、语言可接受性和NLU）表现的同时，在消除偏见方面也是有效的。</p><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>由于两个方面的原因句子级表示的去偏比较困难。首先，对于这些 SOTA 的基于句子嵌入的模型完全重新训练通常是不可行的，这些模型的训练不同于训练词向量只需要花费几个小时，通常它们需要在上百台机器上训练几周时间。当我们发现数据中有一类偏见未考虑到时，重新训练是非常困难的。因此，本文将重点放在事后（post-hoc）去偏技术上，即：在这些句子表示用于下游任务之前，为它们添加一个训练后（post-training）去偏步骤。其次，句子在有单个单词组成的方式上有很大的差异性，这种差异性是由话题、个体、环境、甚至口头和书面文本之间的差异等多种因素导致的。因此，很难讲传统的词级别去偏方法扩展到句子中。</p><p>在本文中，作为将去偏方法推广到句子表示的一个关键步骤，作者捕捉了在自然句子中使用偏见属性词的各种方式。</p><blockquote><p>This is performed by contextualizing bias-attribute words using a diverse set of sentence templates from various text corpora into bias-attribute sentences.</p></blockquote><p>本文提出<strong><em>SENT-DEBIAS</em></strong>方法，为包括性别和宗教在内的二元以及多元偏见属性进行去偏。该方法的关键点是 <em>contextualization</em> 步骤，它通过使用文本语料库中不同的句子模板集，将偏见属性词转化成偏见属性句。实验结果证明了在估计句子表征的偏置子空间时使用大量不同的句子模板的重要性。</p><h4 id="句子表示去偏"><a href="#句子表示去偏" class="headerlink" title="句子表示去偏"></a>句子表示去偏</h4><p>本文提出的 <strong><em>SENT-DEBIAS</em></strong> 句子表征去偏方法由四个步骤构成：1）定义那些表现出偏见属性的词；2）将这些词置于语境中，形成偏见属性的句子以及随后的句子表征；3）估计句子表征的偏置子空间；4）通过去除这个偏置子空间上的投影来消除普通句子的偏见；</p><p>方法如算法1所示：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200918222339.png" alt="image-20200802120630486"></p><ol><li><p><strong>定义偏见属性</strong>：第一步涉及到识别偏见属性和定义一组偏见属性词来表示这些属性。例如，在描绘性别偏见的特征时，使用单词对<em>(man，woman)，(boy，girl)</em>来表示性别。在估计Jewish、Christian、Muslim三类宗教的子空间时，使用元组<em>(Judaism, Christianity, Islam), (Synagogue, Church, Mosque)</em>。每个元组应该包含该偏见属性下具有同等意义的单词。通常来说，对于一个 d 类的偏见属性，数据集中的单词集合 $\mathcal{D}=\{(w_1^{(i)},\dots,w_d^{(i)})\}_{i=1}^m$ 包含 m 个词条，每个词条由一个 d 元组 $(w_1,\dots,w_d)$ 构成，分别代表一种专门的属性。<br><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200918222354.png" alt="image-20200802125928155" style="zoom: 67%;" /><br>现有的研究偏见的方法倾向于在单词级别进行操作，这简化了问题，因为 tokens 的集合是受词汇量限制的。这种简单的方法的有点是利用预定义的词关联集合识别存在的偏见，并利用预定义的偏见单词对估计偏见子空间。然而，句子的潜在数量是无限的，这使得精确地描述存在活不存在偏见的句子变得很困难。因此，直接将这些词转化成句子、从预先训练好的句子编码器中获得一个句子表征并不是一件简单的事情。</p></li><li><p><strong>将单词放入语境中</strong><br><strong><em>SENT-DEBIAS</em></strong> 方法的核心步骤就是将预先定义好的偏见属性单词集根据语境放入到句子中，这样就可以使用句子编码器来获得句子表示。有一种选择是使用简单的基于模板的设计来简化句子编码器于给定的 term 之间的上下文关联性。例如，每个词能够被插入到 <code>This is &lt;word&gt;.</code>，<code>I am a &lt;word&gt;.</code>之类的模板中。本文采用了另一种方法，假设给定的偏见属性（例如性别），一个单个的偏见子空间存在所有可能的句子表征。例如，句子<code>The Boy is coding.</code>, <code>The girl is coding.</code>的偏见子空间应该一样。为了准确地估计这个偏见子空间，应该使用尽可能多样化的句子模板来解释该单词在周围上下文中的所有出现情况。在文中的实验中，作者的经验表明，使用大量的多样化的模板集来估计偏见子空间比使用简单的模板可以提高去偏的效果。<br>为了捕捉句子中语法的变化，作者使用了大规模文本语料库来查找自然出现的句子，这些句子成为了句子模板。为了使用这些模板来生成新的句子，作者将表示单个类的词替换为另一个。例如，一个句子中包含一个男性项 <em>he</em>，它将用于生成新的句子，通过替换将男性项替换成女性项 <em>her</em>。对偏见属性词数据集中所有的单词元组重复这个语境化的过程，最后，将给定的一组偏见属性词至于上下文中形成偏见属性句子。由于每个偏置属性词都有多个模板可以映射，<strong><em>contextualization</em></strong>的过程会产生一个偏见属性句子数据集 $\mathcal{S}$，它的大小要更大：</p><script type="math/tex; mode=display">\mathcal{S} = \bigcup_ {i=1} ^{m} CONTEXTUALIZE(w_1^{(i)},\dots,w_d^{(i)})\\=\{(s_i^{(i)},\dots,s_d^{(i)})\}_{i=1} ^n,|\mathcal{S}|>|\mathcal{D}|</script><p>其中的 $ CONTEXTUALIZE(w_1^{(i)},\dots,w_d^{(i)})$是一个函数，它返回一组句子，这些句子是通过对单词于文本语料库中自然出现的句子模板进行匹配而得到的。<br>文中使用的文本语料库有五个来源：1）<strong>WikiText-2</strong>：正式书写的维基百科的文章；2）<strong>Stanford Sentiment Treebank</strong>：10000篇极化的电影评论；3）<strong>Reddit</strong>：从与政治、电子和人机关系相关的论坛收集的数据；4）<strong>MELD</strong>：一个来自《老友记》中的大规模多模态多方情感对话数据集；5）<strong>POM</strong>：一组来自1000个人的演讲，涵盖多个话题。表2总结了这些数据集：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200918222407.png" alt="image-20200802162548972"></p></li><li><p><strong>估计偏见子空间</strong></p><p>现在我们已经将 $\mathcal{D}$ 所有 $m$ 个偏置属性词条的 $d$-元组情景化后得到的包含 $n$ 个 $d$-元组的句子集 $\mathcal{S}$，我们将这些句子通过预训练的句子编码器来获得句子表征。假设我们有一个预训练的编码器 $M_\theta$，它的参数为 $\theta$，定义 $\mathcal{R}_j,j\in [d]$ 为一个包含 d-元组中所有第 j 个条目的句子表征的集合，$\mathcal{R}_j=\{M_\theta(s_j^{(i)})\}_{i=1}^n$。每个这样的集合定义了一个向量空间，在这个空间中，特定的偏见存在于它的上下文语境中。例如，在处理二元性别偏见时，$\mathcal{R}_1$定义了代表男性（$\mathcal{R}_2$代表女性）的上下文句子表征的空间。$\mathcal{R}_1$与$\mathcal{R}_2$之间唯一的差异就是其中存在的特定偏见属性。<br>定义集合 j 的平均值为 $\mu_j=\frac{1}{|\mathcal{R}_j|}\sum_{\mathrm{w}\in{\mathcal{R}_j}}\mathrm{w}$。偏见子空间$V=\{v_1,\dots,v_k\}$ 是通过PCA得到的前 k 个分量：</p><script type="math/tex; mode=display">V=PCA_k\Bigg(\bigcup_{j=1}^d\bigcup_{\mathrm{w}\in\mathcal{R}_j}(\mathrm{w}-\mu_j)\Bigg)</script><p>k 是超参数，它决定了偏见子空间的维度。直观来看，<strong>V</strong> 表示了最能代表偏见子空间的 top-k 个正交方向。</p></li><li><p><strong>去偏</strong></p><p>给定估计得到的偏见子空间 <strong>V</strong>，我们使用部分版本的 <strong><em>HARD-DEBIAS</em></strong> 算法来去除新句子表征中的偏见，以二元性别偏见为例，<strong><em>HARD-DEBIAS</em></strong> 算法包含两个步骤：</p><ul><li><p><strong>中和(Neutralize)</strong>：通过移除偏见子空间上的投影，从与性别无关且不包含性别偏见的句子中（例如 <em><code>I am a doctor.</code>, <code>That nurse is takign care of the patient.</code></em>) 去除偏见成分。更正式地说，给定一个句子表征 $\mathrm{h}$  以及估计的性别子空间 $V=\{v_1,\dots,v_k\}$，首先获得$\mathrm{h}$ 在偏见子空间 $V$ 中的投影$h_v$ ，去偏的句子表征$\hat{h} = h-h_v$ 。这个结果是一个正交于偏见子空间 $V$ 的向量：</p><script type="math/tex; mode=display">h_v = \sum_{j=1}^k \langle{h,v_j}\rangle v_j</script><script type="math/tex; mode=display">\hat{h}=h-h_v</script></li><li><p><strong>均衡化(equalize)</strong>：性别表示是居中的，其偏见成分是平等的（例如，man 和 woman 应该有相反方向的偏见成分，但是大小相同）。这确保了任何中性词都是在偏见子空间中的偏见词都是等距的。但是由于其实现的复杂性，本文位考虑处理均衡化这一问题。</p></li></ul></li></ol><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>传统的用来衡量偏见的是词嵌入关联测试（WEAT），它通过比较两组目标词和两组属性词来衡量词嵌入的偏见。例如，要衡量围绕职业的性别的社会偏见，可以使用目标词程序员、工程师、科学家、护士、教师、图书管理员，以及属性词man、male和woman、female。无偏的词表征应该使两个目标词在与两组属性词的相对相似性方面没有差异。</p><p>为了评估句子表征中的偏差，这里使用了May等人的方法，将WEAT扩展到句子编码器联合测试中（Sentence Encoder Association Test，SEAT）。对于一个特定的测试，给定一个单词集，使用基于模板的方法将其转化成句子。然后将WEAT应用于固定长度、预训练的句子表征上。</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200918222417.png" alt="image-20200802192704193" style="zoom:67%;" /></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200918222428.png" alt="image-20200802192854353"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200918222432.png" alt="image-20200802192929554"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>对抗过滤数据集中的偏差</title>
    <link href="/2021/06/10/%E5%AF%B9%E6%8A%97%E8%BF%87%E6%BB%A4%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE/"/>
    <url>/2021/06/10/%E5%AF%B9%E6%8A%97%E8%BF%87%E6%BB%A4%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE/</url>
    
    <content type="html"><![CDATA[<h4 id="Adversarial-Filters-of-Dataset-Biases"><a href="#Adversarial-Filters-of-Dataset-Biases" class="headerlink" title="Adversarial Filters of Dataset Biases"></a>Adversarial Filters of Dataset Biases</h4><p>来源：ICML 2020，Allen Institute for Artificial Intelligence</p><p>link：<a href="https://arxiv.org/abs/2002.04108">https://arxiv.org/abs/2002.04108</a></p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>大型神经网络模型在语言和视觉基准测试上已经表现出人类的水平，然而在面对对抗性的或 out-of-distribution 样本时，模型的性能会大幅下降。由此引出一个问题：这些模型是否是通过过拟合虚假的数据集 bias 而学会了解决 <em>dataset</em> 而不是基础的 <em>task</em>。文中研究了一种最近提出的方法，<strong>AFLITE</strong>，它通过对抗过滤这样的数据偏差来减轻普遍高估的机器变现。作者还未 <strong>AFLITE</strong> 提供了一个理论性的理解，通过将其置于最优 bias 减少的广义框架中。文中提出了大量的支持证据，证明 <strong>AFLITE</strong> 广泛使用于减少可测量的数据集bias上，并且在过滤好的数据集上训练的模型对 <strong>out-of-distribution</strong> 的任务有更好的泛化能力。最终，过滤导致模型性能大幅下降（SNLI 从92%下降到62%），但人工的表现依旧很高。因此，本文的工作表明，经过过滤的数据集可以作为升级的 benchmarks，为鲁棒性泛化带来新的研究挑战。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>在很多受欢迎的AI 基准测试上，大规模神经网络已经达到了超人类的表现，例如图像识别（ImageNet）、自然语言推理（SNLI）以及QA（SQuAD）。然而，这些模型在 out-of-distribution 或对抗样本上的表现却下降得很快，这种现象说明最强的 AI 模型的高性能通常局限于特定的数据集，隐式地做出了一个 closed-world 的假设。相反，对一项任务真正的学习需要一个一般化或 open-world 的假设。阻碍泛化的主要障碍是现有的数据集中存在的虚假 bias —— 输入和输出之间的非预期相关性。这样的偏差或人工产物（artifacts）经常在数据收集或人工标注时被引入。数据集偏差不仅不可避免地会使模型产生偏差，而且会显著夸大模型的性能，导致对 AI 系统的真实能力进行高估。</p><p>近期的一些工作已经调查了特定于任务或数据的 bias，包括可视化问答中的语言 bias，ImageNet中的纹理 bias ，以及NLI中的假设依赖问题。这些二研究已经产生了领域特定的算法来解决发现的 bias。然而，这些研究大多遵循自定向下的框架，其中的 bias 消减方法基本上是由研究人员对特定类型的伪偏差的直觉和领域洞察力所引导的。虽然前景看好，但这些方法在根本上受到了算法设计者的人工识别和枚举不想要的bias的限制。</p><p>本文的工作研究了 <strong>AFLITE</strong>，一种选择性的自底向上的算法偏差减少方法。<strong>AFLITE</strong> 是最近提出的一种尽管很简洁但是能够系统地发现和过滤任何众包数据集的 bias 。<strong>AFLITE</strong> 采用了一种基于模型的方法，其目标是消除数据中虚假的人工产物（artifacts），这些人工产物超出了人类能够直观识别的范围，但被强大的模型所利用。下图说明了 <strong>AFLITE</strong> 如何减少 ImageNet 中目标分类的数据集中的bias：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200917142256.png" alt="image-20200916165916314"></p><p>左边为过滤后保留的，右边为滤去的；从热力图可以看到左边的图互相之间的有较大的差异，而右边的相似度很高；经过过滤的例子为图像分类提供了一个更加准确的基准；</p><p>文中首次对 <strong>AFLITE</strong> 进性了理论认识和全面的实证研究。更准确地说，有如下四个贡献：</p><ul><li>首先，作者将 <strong>AFLITE</strong> 用于最优化偏差消减框架中，并证明了 <strong>AFLITE</strong> 提供了 <strong>AFOPT</strong> 的使用逼近，<strong>AFOPT</strong> 是在改框架下理想但是难以计算的偏差消减方法；</li><li>其次，作者提出了<strong>AFLITE</strong> 原文中缺乏的一套广泛的实验，来证明 <strong>AFLITE</strong> 是否真的移除了数据中那些原始假设的虚假偏差；作者的baseline和分析使用了合成的数据集（更易于控制）和真实数据集；后者包括了NLP和视觉任务：SNLI、MultiNLI数据集用于自然语言推理，QNLI 用于问答，ImageNet 用于目标识别；</li><li>三，作者证明了，与在原始有偏差数据集上训练的模型相比，在 <strong>AFLITE</strong> 过滤后的数据集上训练的模型可以更好地泛化到OOD样本上。这些发现表明，数据集中的虚假偏差人为的是基准测试变得容易，因为模型学会了过度依赖这些偏差，而不是学习更多可迁移的特征，从而损害其OOD 泛化能力；</li><li>最后，作者证明了 <strong>AFLITE</strong> 过滤使得那些被广泛应用的 AI 基准变得更具挑战性；实验观察到即使是最先进的模型，它的变现在所有基准上其域内性能都有显著的下降。例如，最好的模型在 <strong>SNLI-AFLITE</strong> 上只达到了 63% 的准确率，大约下降了 30%。这些发现尤其令人惊讶，因为 <strong>AFLITE</strong> 保持了相同的寻来你测试分布，同时保留了一个相当大的训练集。</li></ul><p>总之，<strong>AFLITE</strong> 过滤后的数据集可以作为升级的基准，为模型的鲁棒性泛化提出了新的研究挑战。</p><h4 id="AFLITE"><a href="#AFLITE" class="headerlink" title="AFLITE"></a>AFLITE</h4><p>在大型数据集数据分布的头部有丰富的数据，其性能有更高的优先级，而数据量少的尾部则有折扣。<strong>AFLITE</strong> 试图最小化模型利用分布头部偏差的能力，同时保留尾部固有的复杂性。这一节提供了一个正式的框架来研究这种偏差消减技术，揭示 <strong>AFLITE</strong> 可以看作是最佳偏差消减目标的一种逼近。</p><p><strong>Formalization</strong> 令 $\Phi$ 表示在数据集 $\mathcal{D}=(X,Y)$ 上定义的任何特征表示，<strong>AFLITE</strong> 寻求一个子集 $S\subset \mathcal{D},\ |S|\ge n$能最大限度地适应 $\Phi$ 所揭示的特征；换句话说，对任意同分布的训练集 $S$ 的分割，学习如何最好地利用训练实例上的特征 $\Phi$ 应该不能帮助模型泛化到留存的测试集上。</p><p>令 $\mathcal{M}$ 表示一系列能在 $\mathcal{D} = (X,Y)$ 的子集上使用特征 $\Phi (X)$ 训练的分类模型（例如 逻辑回归、支持向量机或特殊的神经网络结构）。定义关于模型 $\mathcal{M}$ ，$\Phi$ 的表征偏差为 $\mathcal{R}(\Phi,S,\mathcal{M})$，作为 $\mathcal{M}$ 中的模型所能达到的可能的最佳样本外分类精度。给定一个目标最小化缩减的数据集大小 $n$，目标是找到一个大小最少为 $n$ 的子集 $S\subset \mathcal{D}$ ：</p><script type="math/tex; mode=display">\mathop{argmin}_\limits{S\subset \mathcal{D},|S|\ge n}\ \mathcal{R}(\Phi,S,\mathcal{M})</script><p>上式对应的是最佳的 bias 消减，被称为 <strong>AFOPT</strong>，本文中用 $\mathcal{R}(\Phi,S,\mathcal{M})$ 表示期望的分类精度。令 $q:2^S\to[0,1]$ 表示 S 的子集 $T=(X^T,Y^T)$ 的概率分布。接下来以概率 $q(T)$ 随机选择 $T$ ，在 $S \verb|| T$ 上训练分类器 $M_T\in \mathcal{M}$，并在 $T$ 上测试分类器 $f_{M_T}(\Phi(X^T),Y^T)$ 的准确率。$T$ 上的准确率本身是一个随机变量，因为训练集 $S\verb||T$ 是随机采样的。这里将分类准确率的期望值定义为表征偏差：</p><script type="math/tex; mode=display">\mathcal{R}(\Phi,S,\mathcal{M})\triangleq \mathbb{E}_{T\sim q}[f_{M_T}(\Phi(X^T),Y^T)]</script><p>然而，上式中的期望的计算涉及到指数级数量的 $T$ 的不同选择的总和，而且只计算了单个 $S$ 的表征偏差。这使得通过优化式（1）搜寻 $S$ 变得非常棘手。为了绕开这个挑战，我们将 $\mathcal{R}(\Phi,S,\mathcal{M})$ 重构为对实例 $i\in S$ 所有 $T$ 的表示偏差的总贡献。重要的是，这种求和只有 $|S|$ 项，计算更有效。作者将其称之为 $i$ 的 <code>可预测的分数</code>$p(i)$ ，代表了：平均而言，在随机选择的不包含 $i$ 的训练集 $S\verb||T$ 上训练的模型 $\mathcal{M}$ 通过特征 $\Phi(x_i)$ 预测得到的。具有高可预测性得分的实例是不可取的，因为可以利用他们的特征来自信地正确预测此类实例。</p><p>对于每个 $i\in S$，这里将选择一个包含 $i$ 的子集 $T$ 的边缘概率定义为 $q(i)\triangleq\sum_{T\owns i}q(T)$，比率 $\frac{q(T)}{q(i)}$ 是 $T$ 在包含了 $i$ 的条件下的概率。令 $f_{M_T}(\Phi(x_i),y_i)$ 为 $M_T$ 在 $i$ 上的分类准确率，然后，我们从 $p(i)$ 的角度将式(2) 写为：</p><script type="math/tex; mode=display">\begin{align*}&\sum_{T\subset S}q(T)\cdot \frac{1}{|T|}\sum_{i\in T}f_{M_T}(\Phi(x_i),y_i)\\&=\sum_{T\subset S}\sum_{i\in T}q(T)\cdot\frac{f_{M_T}(\Phi(x_i),y_i)}{|T|}\\&=\sum_{i\in S}\sum_{T\subset S \\T\owns i}q(T)\cdot\frac{f_{M_T}(\Phi(x_i),y_i)}{|T|}\\&=\sum_{i\in S}q(i)\sum_{T\subset S \\T\owns i}\frac{q(T)}{q(i)}\frac{f_{M_T}(\Phi(x_i),y_i)}{|T|}\\&=\sum_{i\in S}q(i)\mathbb{E}_{T\subset S,T\owns i}\bigg[\frac{f_{M_T}(\Phi(x_i),y_i)}{|T|}\bigg]\\&=\sum_{i\in S}p(i)\end{align*}</script><p>其中$i$ 的可预测性分数 $p(i)$ 为：</p><script type="math/tex; mode=display">p(i)\triangleq q(i)\cdot \mathbb{E}_{T\subset S,T\owns i}\bigg[\frac{f_{M_T}(\Phi(x_i),y_i)}{|T|}\bigg]</script><p>尽管重构工作支持任意非零的概率分布 $q$，但为了阐述简洁，这里假设 $q$ 是在所有固定大小的 $T\subset S$ 采样上的均匀分布。这使得 $|T|$ 和 $q(i)$ 都是一个固定常量，特别地，$q(i)=\begin{pmatrix}|S|-1\|T|-1\end{pmatrix}/\begin{pmatrix}|S|\|T|\end{pmatrix}=\frac{|T|}{|S|}$，这产生了一种简化的概率分数 $\widetilde p(i)$ 和一个将重构的式(2) 因式分解后的结果：</p><script type="math/tex; mode=display">\widetilde p(i)\triangleq \frac{1}{|S|}\mathbb{E}_{T\subset S,T\owns i}[{f_{M_T}(\Phi(x_i),y_i)}]</script><script type="math/tex; mode=display">\mathcal{R}(\Phi,S,\mathcal{M})=\sum_{i\in S}\widetilde p(i)</script><p>尽管这种重构减少了式(2) 中指数级的求和，使之成为线性和，但是由于 $S$ 有指数级多的选择，求解式(1) 依旧很具有挑战性。然而，重构使得计算效率提升，因此可以通过启发式的方法进行近似计算，以 $S=\mathcal{D}$  开始，不断迭代，通过给定当前的 $S$ 计算出可预测性分数 $\widetilde p(i)$ 最高的实例 $i$ 并将其滤去。<strong>AFLITE</strong> 采用了一种贪婪切片方法，也就是说，它识别出 $k$ 个可预测分数最高的实例，将它们从 $S$ 中移除，并且重复这个过程 $\lfloor \frac{|\mathcal{D}|-n}{k}\rfloor$ 次。这个过程可以看成是对 <strong>AFOPT</strong> 的可计算的实用的近似。附录中，对比了三种启发式方法。所有的情况下，都以一个固定的训练集大小 $|S\verb||T|=t<n$。此外，因为通常需要一个更大的过滤好的集合，因此当每个 $i$ 的可预测分数都低于一个预先设置好的 early stopping 阈值$\tau \in [0,1]$时，我们可以提前结束这个迭代的过程（尽管$|S|>n$）。</p><font face="楷体">再以我的理解归纳一下上述的过程：首先我们要知道最根本的目标是找到$\mathop{argmin}_\limits{S\subset \mathcal{D},|S|\ge n}\ \mathcal{R}(\Phi,S,\mathcal{M})$ ，其中的 $\mathcal{R}$ 是表征 bias，我们需要找到一个 D 的子集，其表征 bias 最小，而 R 的计算需要 S 中以二项分布抽取出子集 $T\subset S$，让模型 M 在S\T 上训练，在 T上测试，计算准确率的期望，这又是一个指数量级的计算。而AFLITE可以算是对这种方法的一种启发式近似，将计算复杂度降低。</font><p><strong>Implementation</strong></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200917142240sadas.png" alt="image-20200917142238893" style="zoom: 67%;" /></p><p>上图中的算法提供了 <strong>AFLITE</strong> 的实现。</p><p>实际使用中，为了效率，$\Phi(X)$ 是提前计算好的，为了获得 $\Phi(X)$ ，我们需要在低数据的情况下根据学习曲线在一小部分数据上训练第一个 ”warm-up“ 模型，在接下来的实验中将不再使用这些数据，并且，这部分数据大小对应于 <strong>AFLITE</strong> 的训练大小 t。$k、m$ 的大小可根据可用的计算预算来决定。</p><p>在每个过滤阶段，在 m 个不同的随机划分的数据上训练模型，并收集它们在对应的测试集上的预测结果。对于每个实例$i$，计算它的可预测性得分，及其标签被正确预测的次数与预测的总次数之比。</p><h4 id="Synthetic-Data-Experiments"><a href="#Synthetic-Data-Experiments" class="headerlink" title="Synthetic Data Experiments"></a>Synthetic Data Experiments</h4><p>作者在合成数据上进行了实验评估 <strong>AFLITE</strong> 能否成功从数据集中移除有可以相关性的数据。作者合成了一个由二维数据点组成的数据集，排列成同心圆，在四个不同的分离层次上，如下图所示：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/202009171529asdas50.png" alt="image-20200917152949956"></p><p>颜色标签代表了数据点所在的圆形区域，显然，一个线性函数并不足以分离这两个类，它需要一个更复杂的分线性模型，例如采用RBF核的SVM。为了模拟数据中的可疑相关性，作者添加了从两个不同的高斯分布中采样的特定类别的人工构造特征(biases)，这些特征只被添加到每个类中75%的数据中，而对于其余的数据，则插入随机特征(噪声)。bias 特征使得任务通过线性函数可解。此外，对于分离程度最大的第一个数据集(图中最左边的)，作者翻转了一些有偏差的样本的标签，使得数据甚至对 RBF 稍微有些不利。两种模型都可以清楚地利用偏差，并在没有偏差的baseline上证明改进的性能。与预期的一样，在应用 <strong>AFLITE</strong> 后，有偏样本的数量就会大大减少，这使得线性模型的任务再次变得困难，但对于非线性模型依然是可以解决的。</p><h4 id="NLP-实验"><a href="#NLP-实验" class="headerlink" title="NLP 实验"></a>NLP 实验</h4><p>SNLI数据集中每个实例都由前提-假设句子对组成，任务包括预测假设是否与前提相关联、是否与前提相矛盾或中性。实验使用 RoBERTa-large 作为特征提取器 $\Phi$，在原始数据随机采样10%的数据上进行训练。构建三个数据集：原始数据集 $D$ ，过滤后的数据集 $D(\phi_{RoBERTa})$， 大小与前者相同但是是随机采样的数据集 $D_{182k}$。</p><p><strong>Out-of-distribution Generalization</strong></p><p>如第1节所述，大规模架构常常通过过拟合数据中输入和输出之间的非预期相关性来解决数据集，而不是去学习解决任务，这种依赖会对更一般化的 OOD 样本造成伤害。之前的一些工作中发现在 SNLI 中存在某些注释的人为产物（artifacts）的存在（例如词汇相关性），这使得当前大多数方法在这个任务上变得很容易。这促使开发出了几个可以仔细地控制之前所说的 artifacts的存在的OOD 测试集。文中的实验在四个这样的数据集上进行了测试：HANS、NLI Diagnostic、Stress tests、Adversarial NLI。考虑到这些基准的收集独立于最初的SNLI任务，因此SNLI中的 bias 不太可能继续存在。下表为在三个数据集上实验的结果，模型在SNLI上预训练，然后直接在目标数据集上进行零样本测试：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200917202721gfdg.png" alt="image-20200917201440945"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/2020091720270fsdf8.png" alt="image-20200917202708687"></p><p>表2中的测试允许评估模型在目标数据上进行fine-tune。</p><p><strong>In-distribution Benchmark Re-estimation</strong></p><p>此外 <strong>AFLITE</strong> 还提供了对多个任务上的基准表现更准确的评估。这里简单调低 early-stop 的阈值 $\tau$，滤去更多的有偏置的样本，剩下 92K个训练样本；</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/2020091721062das0.png" alt="image-20200917210620539"></p><p>第一个基线使用给定实例中的单词和目标标签之间的点互信息(Point-wise Mutual Information, PMI) 作为其唯一特征，因此它能捕捉到数据中存在的词关联bias的程度；HypOnly 代表只基于假设；</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/2020091721330dsaa0.png" alt="image-20200917213032498" style="zoom:50%;" /></p><h4 id="视觉实验"><a href="#视觉实验" class="headerlink" title="视觉实验"></a>视觉实验</h4><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200917213246das.png" alt="image-20200917213246790"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/2020091721333das3.png" alt="image-20200917213333776" style="zoom:67%;" /></p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p><strong>Adversarial Filtering</strong> 对抗过滤算法与AFLITE很相关，区别主要是两点：1) 用处更广泛（不需要过渡生成数据实例）；2）明显更轻量（每次迭代不需要重新训练模型）；各种 AF 算法的变体被用于创建其它数据集，通过迭代扰动数据集样例，知道目标模型无法匹配结果数据集。这些方法虽然有效，但有三个主要缺陷：1）数据集监管者需要明确的设计一种策略来为给定的实例收集或生成扰动；2）这些方法存在分布bias的风险，鉴别器能够学会区分机器生成的和人类生成的实例；3）每一次迭代都要重新训练模型，计算量大。</p><p>AFLITE最初是今年的一篇论文中提出来用于生成 Winogrande数据集的。本文给出了更全面的实验、理论证明以及应用推广。</p><p><strong>Data Selection for Debiased Representation</strong> 2019有一篇工作 REPAIR，通过数据重采样来移除表征bias，它的动机是学习数据集上的概率分布，这种分布有利于那些对于给定的表征来说很难的实例。对比于AFLITE，REPAIR的使用依赖于寻来你中的分类loss，而不是样本外的泛化准确率。RESOUND（2018）量化了数据集的表征bias，并通过对现有的 C 类数据集进行采样，用它们来装配一个偏差更小的新的K类数据集；数据蒸馏（2018）：它的目的是合成少量的实例来逼近在原始数据上训练的模型；</p><p><strong>Learning Objectives for Debiasing</strong>  另一个相关的方面是通过设计学习目标来移除数据表征中的bias；将不变风险最小化（Invariant Risk Minimization）作为促进学习跨环境稳定的数据表征的目标；Belinkov等人(2019)提出了一种对抗去除技术，鼓励模型学习不带假设偏差的表征；He等人(2019)提出了 DRiFt，一种去偏算法，该算法首先使用已知的有偏特征学习一个有偏模型，然后训练一个符合有偏模型残差的去偏模型。类似地，Clark等人(2019)提出学习仅使用偏置特征的朴素分类器，与包含更一般特征的其他分类器一起用于集成。Elazar &amp; Goldberg(2018)的研究表明，对抗训练有效地缓解了人口统计学信息泄漏，但在处理文本数据时并不能完全消除人口统计学信息泄漏。</p><h4 id="Discussions"><a href="#Discussions" class="headerlink" title="Discussions"></a>Discussions</h4><font face="楷体">这篇文章对AFLITE算法进行了一个论证，针对AFLITE算法，是否可以有一个改进，使之能用于对无监督数据的遴选？文中的方法在debias问题上没能显式找出bias的存在，仅是自动滤去一些可能存在可以相关性的数据，有没有更针对性的方法能去偏但不用滤去数据，甚至可以显式找出这些bias？如文中所述，这个方法为深度学习中很多任务提出了新的benchmark，是否可以 follow 这项工作应用到NLP其它领域中，ACL2020上有一篇引用了这篇文章的论文（[Adversarial NLI: A New Benchmark for Natural Language Understanding](https://arxiv.org/pdf/1910.14599.pdf)），是否可以有一些启发？</font>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>A survey of bias in NLP</title>
    <link href="/2021/06/10/A-survey-of-bias-in-NLP/"/>
    <url>/2021/06/10/A-survey-of-bias-in-NLP/</url>
    
    <content type="html"><![CDATA[<h3 id="Mitigating-Gender-Bias-in-Natural-Language-Processing-Literature-Review"><a href="#Mitigating-Gender-Bias-in-Natural-Language-Processing-Literature-Review" class="headerlink" title="Mitigating Gender Bias in Natural Language Processing: Literature Review"></a>Mitigating Gender Bias in Natural Language Processing: Literature Review</h3><p>ACL 2019 <a href="https://arxiv.org/ftp/arxiv/papers/1906/1906.08976.pdf">https://arxiv.org/ftp/arxiv/papers/1906/1906.08976.pdf</a></p><font face="楷体">这篇是ACL 2019 上的关于性别 Bias 的综述，内容上主要是对问题、方法和数据集进行了介绍，能够帮助理解性别 bias、刻板印象（stereotpyes）等概念以及19年之前的一些debias方法。内容比较简单易懂，这里仅做简单概述，有兴趣可以读读原文。</font><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文综述了当前NLP中认知和消除性别 bias 的研究，从四种表现形式来讨论性别 bias，此外，文中还讨论了现有的 debias 方法的优缺点，最后讨论了未来的研究，以重新认知和减轻 NLP 中的性别 bias。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>在 NLP 系统中，性别 bias 表现在多个部分，包括训练数据、资源、预训练模型和算法本身之中。在任意这些部件中包含 bias 的 NLP 系统都会产生性别有偏的预测，有时候甚至会扩大训练集中的 bias。性别 bias 在NLP算法中传播有可能在下游任务中会强化有害的刻板印象，这对现实世界会造成影响，例如有人担心在简历自动筛选系统中，当唯一的区别因素是应聘者的性别时，会优先考虑男性应聘者。</p><p>本文中也引入了分配性偏差和表征偏差，前者可以被定义为一种经济现象，系统不公平地将资源分配给某些群体，后者发生在当系统偏离了特定群体的社会身份和表征时。就应用来看，当模型在与主要性别相关的数据上的表现更好是就反映出分配性bias；当word embedding 和模型参数中捕获到性别与某些概念之间的关联时，就反映出表征 bias；</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200918215618.png" alt="image-20200918092417062"></p><p><strong>诋毁（Denigration）：</strong>指得是在文化或历史上使用贬低的术语；<strong>刻板印象（stereotyping）：</strong>加强了社会中已经存在的刻板印象；<strong>认知（recognition）：</strong>认知偏差是指在识别任务给定算法的不准确性；<strong>under-representation：</strong> 指某特定群体表征上不成比例地低；</p><h4 id="Observing-Gender-Bias"><a href="#Observing-Gender-Bias" class="headerlink" title="Observing Gender Bias"></a>Observing Gender Bias</h4><p>介绍了三类评估 bias 的方法：</p><ul><li><p>Adopting Psychological Tests</p><p>将一种名为 隐性关联测试（Implicit Association Test）的新理测试方法引入到词(Word Embedding Association Test, WEAT)和句子 (Sentence Encoder Association Test, SEAT) embedding 的 bias 检测中；</p></li><li><p>从embedding来分析性别子空间</p><p>包括16年的 <strong>Hard Debias</strong>，今年的<strong>Double-Hard Debias</strong>、<strong>SENT-DEBIAS</strong>，都是考虑从词或句子的embedding中移除性别子空间的分量的角度来进行事后去偏；这类方法就是通过定义一些性别词对（<code>she-he, her-his, girl-boy, woman-man</code>) 来找出这个子空间。</p></li><li><p>不同性别不同表现</p><p>通过将测试数据中的单一性别替换成另一性别，得到句子对，去测试原始数据集和性别交换数据集上模型表现的差异；</p><p>NLP 的标准评估数据集在衡量性别 bias 方面是不相等的，首先这些数据集通常也包含 bias，因此在这些数据集上的评估可能无法揭示性别 bias，此外，执行复杂的NLP任务的系统所做出的预测取决于多种因素，因此必须仔细设计数据集，隔离性别对输出的影响，以便能够探测性别bias，文中将这类数据集命名为<strong>性别偏见评估测试集（GBETs）</strong>；</p></li></ul><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200918215635.png" alt="image-20200918103800447"></p><h4 id="通过数据操纵-debias"><a href="#通过数据操纵-debias" class="headerlink" title="通过数据操纵 debias"></a><strong>通过数据操纵 debias</strong></h4><p>有两种，一种是带着 debias 的目的去重训练模型，另一种是事后去偏，仍利用原始的 embedding，但是在训练过程中以及测试过程中对现有模型进行修补，以调整其输出；</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/2020091811081das4.png" alt="image-20200918110814671"></p><h5 id="为训练语料去偏"><a href="#为训练语料去偏" class="headerlink" title="为训练语料去偏"></a>为训练语料去偏</h5><ol><li><p><strong>数据增强：</strong>通过性别交换创建一个性别平衡的数据集来试图消除模型的预测偏差（GBETs是用来评估去偏前后模型表现差异的），数据增强的工作流程如下：对于原始数据集中的每个句子，创建一个相同的但性别交换了的句子；接下来对每个原始句子以及与之对应的性别替换句子进行匿名化；匿名化即用匿名的实体 ”E1“ 来替换所有命名实体，例如 <em>Mary likes her mother Jan</em> 在性别交换以及匿名化处理后变成 <em>E1 likes his father E2</em>。这就移除了句子中命名实体和性别之间的联系，在原始数据和增强数据结合的数据集上对模型进行训练。识别性别专用词和对应的性别词通常需要手动创建列表。</p></li><li><p><strong>性别标记：</strong>在某些任务中，如MT，混淆数据点来源的性别可能导致不准确的预测，因为训练集以男性数据为主，所以模型学习了扭曲的统计关系，因此当性别来源不明确时，更有可能预测说话者是男性。性别标记是通过在每个数据点的开始出添加一个表示数据点来源的性别标记来减轻这一问题。例如，”I’m happy” 被标记成 ”MALE I’m happy.” 从理论上讲，在句子中对性别信息进行编码可以提高译文的质量，模型可以将标记与其它数据分开进行解析，保留原句中的性别信息。</p></li><li><strong>Bias Fine-Tuning</strong>：给定任务的无 bias 数据可能很稀缺，但是其相关任务可能存在无 bias 数据，Bias Fine-Tuning 结合了对无偏数据集的迁移学习，以确保模型在使用目标任务上有偏的数据集进行 fine-tuning 之前，模型能尽可能包含最小的 bias；</li></ol><h5 id="为词-embedding-性别去偏"><a href="#为词-embedding-性别去偏" class="headerlink" title="为词 embedding 性别去偏"></a>为词 embedding 性别去偏</h5><ol><li><strong>移除 embedding 中性别子空间中的分量</strong></li><li><strong>学习性别中立的词 embedding</strong></li></ol><h4 id="通过调整算法去偏"><a href="#通过调整算法去偏" class="headerlink" title="通过调整算法去偏"></a>通过调整算法去偏</h4><p><strong>限制预测</strong></p><p>NLP模型有放大bias的风险，(Reducing Bias Amplification) RBA 方法限制其模型优化函数，以确保期预测符合限定条件。例如，当RBA用于预测视觉语义角色标签时，它限制了预测从事特定活动的男女比例，以防止模型放大bias。</p><p><strong>对抗学习：调整鉴别器</strong></p><p>对传统GAN进行改动，让生成器对受保护的性别属性进行学习。换句话说，在给定的任务中，生成器试图阻止鉴别器识别性别，这种方法比较容易泛化：因为它可以用来消除任何使用基于梯度学习的模型的偏差。</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>目前的方法存在一些局限性：1. 大多数 debias 技术几种在端到端 NLP 系统的单一的模块化的过程上，这些单独的部分如何聚集起来形成一个理想的无偏系统还有待发现；2. 大多数 debias 方法尽在优先的应用中得到了实验验证，不清楚能否推广到其它任务或模型上；3. debias 技术也许会引入一些噪声，大致性能下架；4. 手工设计的 debias 方法会无意间隐含开发者的 bias；</p><hr><hr><h3 id="Language-Technology-is-Power-A-Critical-Survey-of-“Bias”-in-NLP"><a href="#Language-Technology-is-Power-A-Critical-Survey-of-“Bias”-in-NLP" class="headerlink" title="Language (Technology) is Power: A Critical Survey of “Bias” in NLP"></a>Language (Technology) is Power: A Critical Survey of “Bias” in NLP</h3><p><strong>介绍</strong>：<font face="楷体">这篇文章是 ACL 2020 上的一篇文章，文中对研究NLP领域的 “bias” 问题的146篇论文进行了归类，并指出这些文章在研究 “bias” 问题时的不规范论证、彼此不一致的问题，之后作者从社会学、语言学等领域更深而广的层次呼吁后续的研究者能够将这一领域的研究规范化，并提出了三条建议以及一些相关研究问题。</font></p><p><a href="https://aclweb.org/anthology/2020.acl-main.485.pdf"><strong>原文链接</strong></a></p><h4 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文中，作者调研了146篇分析 NLP 中的 “bias“ 问题的论文，发现他们的动机是含糊不清、不一致的，缺乏规范的推理论证，尽管分析 “bias” 是一个固有的规范过程。他们进一步发现这些论文提出的评测、消除 “bias” 的方法与他们的动机并不匹配，并且他们没有接触 NLP 以外的文献。基于这些发现，作者提出了三个建议来指导在 NLP 系统中分析 “bias” 的工作。这些建议基于对语言和社会等级之间的关系的更宽广的认识，鼓励研究者和实践者明确表达他们关于 ”bias“ 的概念认知，例如，系统的哪些行为是有害的，以什么方式、对谁有害、为什么？并以受NLP系统影响的社区成员的生活经历为中心展开工作，同时质疑和重新设想技术人员和此类社区之间的权力关系。</p><h4 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h4><p>近些年出现了大量分析NLP系统中的 “bias” 的工作，以及包括语言建模、指代消解、机器翻译、情感分析、仇恨词检测在内的各种任务的系统中研究 “bias”。</p><p>尽管这些文章通过说明 NLP 系统中的某些方式有害，奠定了重要的基础，但是大多数工作都未能从以开始就批判性地探讨是说明构成了 ”bias“。尽管偏见分析是一个固有的规范过程，在这个过程中，有些系统行为被认为是好的，有些则是有害的，但是讨论关于 NLP 系统中的 ”bias” 论文总是充斥着未明确说明的假设，这些假设是关于哪些系统行为有害、以何种方式造成损害、对谁有损害、为什么等方面的。事实上，“bias” (或性别偏见或社会偏见) 被用来描述一系列系统行为，即使它们可能以不同的方式、对不同的群体或出于不同的原因造成了损害。即使是相同任务的NLP系统中，不同的论文在分析 “bias” 时，也常常定义出不同的概念。</p><p>例如，以下的系统行为都可以不言而喻地理解为 ”种族歧视“：a) 在 embedding 空间中，与非裔美国人相关的名字的 embedding (与欧洲裔美国人相关的名字对比)距离令人不愉快的词要比令人愉快的词更近；b) 对于包含与非裔美国人有关的名字和包含与欧裔美国人有关的名字的句子，情绪分析系统得出不同的强度分值；c) toxicity detection system 在评估包含于非裔美国人英语相关特征的推文时，得到的结果相比于没有这些句子的推文更具攻击性。此外，这些论文有的侧重于书面文本中表达的种族偏见，有的则关注针对作者的种族偏见。使用不精确的术语掩盖了这些重要的区别。</p><p>作者调研了146篇分析 NLP 中的 “bias” 的论文，发现它们的动机经常是模糊不清而且不一致的，许多论文缺乏规范的论证过程，此外，当提出莲花技术来衡量或减轻 ”bias“ 时，大多数论文都没有涉及到 NLP 之外的相关文献来确定规范的关注点，结果是许多方法于它们的动机并不匹配，而且彼此之间也不具有可比性。</p><p>之后，作者提出了三个指导分析 NLP 中的 “bias” 的建议，作为这一方向前进道路的开端。作者任务，这一工作应该探究语言和社会阶级之间的关系，并呼吁从事这类工作的研究人员和从业者阐明他们关于 “bias” 的概念，以便能够就哪些系统行为是有害的、以何种方式、对谁、为什么进行讨论，并且作者建议技术人员和受 NLP 系统影响的社区之间进行更深入的交流。</p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p>作者调研的146篇论文是从计算机语言学相关领域国际会议中找出来的，论文都是2020年5月之前的论文，主要针对社会偏见问题的定义、度量与消除，下表是这146篇论文中涉及的NLP任务及对应论文数量：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200821150849.png" alt="image-20200821150842055"></p><p>作者根据一些工作中开发的伤害分类方法对这些论文进行了分类，区分了所谓的分配性 (allocational) 伤害和表征性 (representational) 伤害。分配性伤害会引起自动化系统在分配资源或机会的时候对不同的社会群体做出不公平的行为，表征性伤害出现在当系统以不太有利的方式代表某些社会群体，贬低他们，或者完全不承认他们的存在。根据动机和所提出的技术，作者将这些论文分为以下几类：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200821164153.png" alt="image-20200821164153171"></p><ul><li>表征性伤害：<ul><li>Stereotyping：传播对特定社会群体的负面概括的刻板印象；</li><li>其它表征性伤害：不同社会群体在系统表现差异，歪曲不同社会群体在人口分布上的语言，贬低特定社会群体的语言；</li></ul></li><li>Questionable Correlations：系统行为和语言特征之间的可疑相关性，通常与特定的社会群体有关；</li><li>Vague：对于“bias” 的概念表述含糊不清或完全没有进行描述；</li><li>调研、框架和元分析；</li></ul><p>在表2中，作者提供了上述分类的计数 (附录中提供了每个类别的论文列表) ，附录中的表3包含了这些论文在一些列不同的 NLP 任务中的动机和技术的例子。</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200822102957sad.png" alt="image-20200822102856067"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/202008231640da41.png" alt="image-20200822103046249"></p><h4 id="Findings"><a href="#Findings" class="headerlink" title="Findings"></a>Findings</h4><p>作者在分类后，发现了这些论文中的一些共性，并进行了讨论：</p><h5 id="Motivations"><a href="#Motivations" class="headerlink" title="Motivations"></a>Motivations</h5><p><strong>论文阐述了广泛的动机、多重动机、模糊的动机，有时候根本没有动机。</strong> </p><p>作者发现，这些论文的动机涵盖了所有六种类别，每一种类型都有几篇论文，那些分析 NLP 系统中的 “bias” 提供综述或框架的论文中通常会陈述多种动机。然而，如同表3中举的例子，许多其它论文 (33%) 也这样做了，有些论文(16%) 指陈述了模糊的动机或根本没有动机，例如：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200821170441.png" alt="image-20200821170440994"></p><p>这些例子没有说明NLP系统的“歧视”可能意味着什么，什么构成了“系统性偏见”，或者NLP系统如何促成“社会不公正”(本身没有定义)。</p><p><strong>论文的动机有时包含了不规范的论证。</strong></p><p>作者发现，有些论文(32%) 没有根据显著规范的关注点来寻找动机，而是关注在系统性能上。例如，下面的第一段引用包含了规范的论证，即模型不应该使用人口统计信息来做预测，而第二段引用则侧重于有损系统性能的学习相关性。</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/2020082210421as2.png" alt="image-20200822104212238"></p><p><strong>即使论文明确地陈述了动机，但也常常不清楚为什么被描述为 “偏见” 的系统行为是有害的，以什么方式，对谁有害。</strong></p><p>作者发现即使论文中有清晰的动机，但却没能说清那些系统行为是有害的，以什么方式、对谁、为什么，例如：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200822105205sada.png" alt="image-20200822105117051"></p><p>这些例子没有说清楚 “problematic bias” 或非理想的用户体验可能是什么样子的，系统的行为如何导致这些事情的，以及相关的涉众或用户可能是谁。相反，作者发现那些提供了调研或框架的论文在分析这些问题的时候，往往指出了谁受到了伤害，承认不同的社会群体由于与 NLP 系统的不同关系或不同的社会地位，得到的用户体验也可能不同，例如有一篇文章中就主张在涉及对话代理时，应该对用户群体的特上下文和兴趣有深入的理解。</p><p><strong>为同一任务而开发的 NLP 系统的论文常常对 “偏见” 有不同的论述。</strong></p><p>有些论文即使是讨论同一 NLP 任务，对于 “bias” 的概念却有本质上的不同，有时甚至是不一致的。表3中的第3、4行包含了不同文章中对于 机器翻译任务中 “bias” 的不同定义，导致了两篇文章提出了不同的技术，第5、6行包含了关于 embedding 空间中的 “bias“ 的论文，他们分别陈述了不同的东起，却提出了量化刻板印象的技术。</p><p><strong>论文的动机将分配伤害和表征伤害混为一谈。</strong></p><p>作者发现，有些论文(16%) 的动机提到了直接的表征性伤害，例如刻板印象，以及更远的分配性伤害，在刻板印象的情况下，分配伤害通常被认为是刻板印象对下游任务的影响。这类论文通过想象的下游效应来证明关注特定的系统行为是正确的，即使他们根本就没有对下游效应进行测量。探究嵌入空间中的 ”bias“ 的论文游戏喜欢这样做，因为 embedding 经常被用作其它系统的输入：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200822124152asdaq.png" alt="image-20200822124152561" style="zoom:67%;" /></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/20200822124542.png" alt="image-20200822124251822"></p><p>相反，那些提供调研或框架的论文以他们的方式将这些问题阐述清楚了。</p><h5 id="Techniques"><a href="#Techniques" class="headerlink" title="Techniques"></a>Techniques</h5><p><strong>论文中谈到的技术没有在 NLP 之外的相关文献中找到很好的理论基础。</strong></p><p>论文的动机常常很模糊、彼此不一致、缺乏规范的论证，这也许不令人惊讶，作者还发现，论文中提出的用来衡量或减轻 ”bias“ 的量化技术并不能有效地与 NLP 领域之外的相关文献产生很好的联系。</p><p>研究刻板印象的论文是一个值得注意的例外：WEAT (Word Embedding Association Test) 使用了一种隐式关联测试的方法，它来自于1998年的一篇社会心理学文献中的内容，此外还有一些研究种族和性别偏见的文章也借鉴了黑人女权主义的交叉研究。</p><p><strong>论文中的技术与他们的动机并不匹配。</strong></p><p>作者发现大约21%的论文的研究动机种包含了分配损害，但只有四篇文章真正提出用于衡量或减轻分配性损害的技术。</p><p><strong>论文关注的点是一个狭窄范围内潜在的 “bias” 来源。</strong></p><p>作者发现几乎所有的论文都把系统的预测作为 ”bias“ 的潜在来源，此外，还有人关注数据集中的 ”bias“（例如，训练数据中性别代词数量的差异）。大多数论文没有细究在开发和部署周其中做出其它决策的规范性含义，考虑到他们的动机有时候也不包含规范性论证，这也许并不令人惊讶。少数论文是例外，他们说清楚了任务的定义、注释指南以及评估指标。</p><h4 id="A-path-forward"><a href="#A-path-forward" class="headerlink" title="A path forward"></a>A path forward</h4><p>作者在这一节提出了指导 ”bias“ 研究工作的三条建议，并为每条建议都提供了几个具体的研究问题，作者强调这些问题并不全面，器目的是引出进一步的问题于研究路线：</p><ol><li><font color=blue size=4>在探究语言与社会等级关系的 NLP 领域外的相关文献中，分析自然语言处理系统中的 “bias”，将表征性伤害本身视为有害的。</font>)作者认为，引用 NLP 领域外的相关文献能够使得这一研究更加全面，许多学科，包括社会语言学、语言人类学、社会学和社会心理学，研究语言如何承担社会意义，以及语言在维持社会等级中的作用。<font face="STCAIYUN">接下来作者举了很多 NLP 之外的相关领域的文献中关于社会中存在的”bias“ 问题的例子，这里不列举了。</font><p>认识到语言在维持社会等级中的作用，对于分析 NLP 系统中 “bias” 未来的研究至关重要，首先，它有助于解释为什么表征伤害本身就是有害的；其次，语言和社会层级之间的复杂关系说明了为什么研究 NLP 系统中的 “bias“ 如此具有挑战性，这意味着研究人员和从业者将需要超越现有的算法公平性技术。作者认为，如果不与 NLP 之外的相关文献作为基础来开展研究工作的话，研究人员和实践者可能会只去度量或减轻那些方便度量或减轻的 ”bias“，而不是最规范的关注点。</p><p>更具体地说，作者建议分析 NLP 中 ”bias“ 的工作应该围绕以下问题重新定位社会等级制度、语言以实形态和 NLP 系统是如何共同生产的？研究人员和时间着应该询问现有的社会等级和语言以实是如何驱动 NLP 系统的开发和部署的，以及这些系统是如何复制这些等级和意识的。作为围绕这个问题重新定位 NLP 系统 ”bias“ 分析工作的起点，作者提供了几个具体的研究问题：</p><ul><li><p><strong>在开发和部署周期中，社会层次结构和语言意识如何影响决策？这些决策会导致什么样的NLP系统，它们会排除掉哪些特征？</strong></p><ul><li>一般性假设：NLP 系统应该遵循哪些语言规范？哪些语言时间被默认为是标准的、普通的、正确的或适当的？</li><li>任务定义：NLP 系统是为哪些人开发的？任务定义如何 discretize the world？例如，在处理人口统计属性预测任务时，如何划分社会群体？那母语预测任务中的语言呢？</li><li>数据：数据集是如何手机、预处理、标记或注释的？注释指南、注释者的假设和观点的影响是什么，以及注释聚合过程？</li><li>评估：NLP 系统是如何评估的？评价指标的影响是什么？是否进行了非定量的评估？</li></ul></li><li><p><strong>NLP 系统如何重现或转化以实形态？那种语言变体或实践被认为是好的或不好的？好的语言是不是以为着它更容易被现有的 NLP 系统所掌握？例如，许多语言实践产生的语言现象被称为 ”noisy text“，并且被当作是正则化的目标。NLP 系统所复现的语言意识形态是如何维持社会层级的？</strong></p></li><li><p><strong>哪些表征性损害正在被衡量或减轻？这些二是最符合规范的损害，还是说仅仅是因为现有的算法公正系统更容易处理这类问题？还有其它可分析的表征性伤害吗？</strong></p></li></ul></li><li><font color=blue size=4>提供明确的表述，说明为什么被描述为 “bias” 的系统行为是有害的，以什么方式造成损害，对谁有害。坦率地说出这些陈述背后的规范性论述。</font><p>换句话说，研究者和实践者应该清楚地表明他们对 ”bias“ 的理解，正如前文所述，论文在描述系统行为时，经常对 ”bias“ 使用不言自明的陈述，这种不精确术语的使用导致了所有论文都声称要分析 NLP 中的 ”bias“，有时甚至是为相同任务而开发的系统，但对于 “bias” 却有着不同甚至不一致的概念，并且没有对这些差异进行解释。</p><p>然而，分析 “bias” 是一个内在的规范过程，在这一过程中，一些系统行为被认为是好的，另一些则是有害的，但是很多论文却没有将其阐述清楚。作者呼吁研究员和实践者通过阐明那些他们价值观觉得有害的系统行为，来明确他们的规范论证，无论这些价值观看起来多么明显。</p><p>作者进一步认为，这些论证应该考虑到之前提到的语言和社会层级之间的关系。首先，这些关系提供了一个基础，再次基础上，我们可以更好地进行规范论证。例如，有些系统行为可能是有害的，正是因为它们维持着社会层级。其次，如果分析 “bias” 的工作被重新调整为弄明白社会层级、语言意识形态和NLP 系统是如何共同运作的，并且如果我们不能从以开始就考虑到社会层级和语言意识形态是如何决定我们所说的 “bias” ，那这项工作将是不完整的。为此，作者提出以下具体研究问题作为切入点：</p><ul><li><strong>哪种系统行为被描述为 ”bias“ ？它们潜在的来源是什么（例如，性别假设、任务定义、数据）？</strong></li><li><strong>这些系统行为以何种方式造成损害，对谁，为什么？</strong></li><li><strong>哪些社会价值观（明显或不明显）加强了这种 ”bias“ 的概念？</strong></li></ul></li><li><font color=blue size=4>通过实际生活中受 NLP 影响的社区成员的生活经历来检验语言在实际中的使用，询问并重新构想技术人员和这样的社区之间的关系。</font><p>最后，作者的观点建立在对语言和社会层级关系之间的更宽广的认识上，提出了在实践种检验语言使用的几个方向。这里主要关注两点，首先，因为语言的位置是必然的，因为不同的社会位置的社会群体有不同的生活经历，特别是储于多个压迫加差点的群体，作者建议研究人员和从业者集中分析那些生活经历受到这些 NLP 系统影响的社区成员，分析这里面的 NLP 系统中的 ”bias“；其次，作者建议对技术人员和这类群体之间的权力关系进行审查和重新构想。研究人员指出，算法公正技术通过提出递增的技术性缓解（例如，搜集新的数据集或训练更好的模型），通过假设自动化系统应该继续存在而不是询问它们是否应该被建立，以及将开发和部署的决策维持在技术人员的掌握中，来维持这种权力关系。</p><p>在研究这些方向时，有许多学科可供研究人员和实践者借鉴。<font face="STCAIYUN">作者这里列举了一些相关领域的研究成果和研究动向。</font></p><p>作为接触受 NLP 系统影响的社区的七点，作者提供了以下具体的研究问题：</p><ul><li>社区如何发觉 NLP 系统的?他们会抵制吗?如果是，是怎么抵制的?</li><li>如果 NLP 系统不能很好地工作，社区要承担哪些额外的成本？</li><li>NLP 系统是否会将权力转移到压迫性的机构（例如，通过实现社区不想要的预测、基于语言的资源或激活的不公平分配），还是远离这些机构？</li><li>谁参与了 NLP 系统的开发和部署？决策过程如何维持技术人员和受 NLP 系统影响的社区之间的权力关系？这些过程能被改变，用来重新构想这些关系吗？</li></ul></li></ol><h4 id="Case-study"><a href="#Case-study" class="headerlink" title="Case study"></a>Case study</h4><p>为了距离说明这些建议，作者提供了一个关于非洲裔美国人英语 (AAE ~这种语言变体有许多不同的名称，现在通常被称为非裔美国英语、非裔美国方言AAVE或非裔美国语言AAL~) 的案例研究。</p><p>对 AAE 的上下文的 “bias” 分析表明，词性标注、语言识别系统和依存分析的效果在包含 AAE 相关特征的文本上都不如没有这些特征的文本上好，并且 toxicity detection 系统将含有 AAE 相关特征的推文在攻击性的得分上要高于不含 AAE 特征的推文（这里引用了四篇论文）。这些论文对于强调 AAE 是一种语言变体起到了至关重要的作用。然而，他们没有以同样的方式将 ”racial bias“ 概念化，四篇论文中第一篇仅关注包含 AAE 相关特征的文本和不包含这些特征的文本在系统性能表现上的差异。相比之下，后两篇论文也关注了系统性能的差异，但是通过以下额外的推理来激发这种关注：如果包含于 AAE 相关特征的推文被认为比不包含这些特征的推文耕局攻击性，那么这可能 a) 对AAE 产生负面看法；b) 导致包含这些特征的推文被不成比例地移除，阻碍了发言中参与在线平台，自由使用 AAE 进行发言的空间； c) 导致 AAE 说话者不得不改变他们的语言习惯，以避免受到负面看法或被推文被删除，这使得他们承担了额外的花费；</p><p>更重要的是，这些论文都没有涉及到关于 AAE 的文献，及美国的种族等级制度以及种族语言学的意识形态。由于没有接触到这些文献，因此将 AAE 简单地当作许多 <strong>non-Penn Treebank varieties of English</strong> 中的一种，在 AAE 的上下文中分析 NLP 系统的 ”bias“ 无法在世界上定位这些系统。谁是 AAE 的使用者？他们如何看待？作者认为，AAE 作为一种语言变体，不能和它的使用者(主要是经历了系统性的烦黑人种族主义的美国黑人群体) 分离，并且语言意识形态强化和证明了种族等级制度。</p><p>即使经过社会语言学家几十年的努力使 AAE 合法化，但是它仍被认为是不好的英语，并且它的使用者依旧被视为语言上有缺陷 (一种被称为 deficit 的观点)。这种观点无视 AAE 已经具有规则约束和语法约束，这种观点属于更广泛的种族语言意识形态，这也可能会造成分配性伤害；AAE 的使用者经常因为不遵守主流语言习惯而受到惩罚，包括教育系统中，当寻找住所或在司法系统中，他们的证词经常被误解，或者更糟，直接被怀疑。这些种族语言意识形态认为，种族化的社区需要语言干预，如语言教育项目，在这些项目中，如果社区适应主流语言实践，这些伤害和其他伤害就可以减少。$\cdots$</p><p>通过阅读上述文献，我们可以发现那些被称为 ”bias“ 的系统行为，为规范论证提供基础。研究者和从业者应该关注 toxicity detection 系统中的种族偏见，不仅仅因为性能差异会损害系统性能，而且要考虑到它们重现长起对 AAE 使用者的歧视和权力被剥夺的不公正性。在重新污名化 AAE 的过程中，它们在线了语言意识形态，在这些意识形态中，AAE 被认为是不符合语法的、未教化的甚至是无礼的。反过来，这些意识形态又长期助长了语言其实，并未长期存在的种族等级制度提供了理由。</p><p>作者强调，接触有关 AAE、美国种族等级以及种族以实形态相关的文献，可以产生新的关联路线。这些路线包括了在开发和部署 NLP 系统的过程中避免产生歧视和剥夺权力的方法，并致力于在实践中使用 AAE。本文献还可以帮助研究者和从业者解决NLP系统可能产生的配置危害，并确保即使是善意的NLP系统也不会将种族化社区定位为需要语言干预或适应主导语言实践的群体。最后，那些希望设计初更好的系统的研究人员和实践者还可以借鉴越来越多的反种族主义语言教育法，以及前文叙述的关于重新构想技术人员和受影响的社区之间的权力关系的工作。</p><h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>作者通过调查146篇分析 NLP 系统中的 “bias” 的论文，发现了这些论文的动机通常含糊不清，缺乏规范的推理，他们提出的技术和他们的动机不匹配，并且没有接触到 NLP 之外的相关文献。之后作者提出了三个建议来指导这一领域的研究工作的后续进行。</p><font face="楷体">    之前我在这方面的调研工作中，也感受到了本文中作者提到的一些问题，不同的论文对于 “bias” 的定义存在差异，甚至彼此矛盾，有些论文认为是数据分布所致，有些论文认为是机器学习到了人类社会中的刻板印象，而他们提出的解决方法也有很大的差异。这篇综述，最大的贡献除了所提出的规范化建议，还有一点就是对之前的146篇论文的一个归类，六种类别的划分，尤其是区分开了分配性损害和表征性损害，后续的研究应该细致区分开这两点。</font><font face="楷体">    此外，作者提出的建议中的第一点-结合其它领域的相关文献和第三点-要求和NLP系统中“bias”的受损害人群进行沟通，这一点可能太过超前，我觉得从NLP技术的角度来看，能够做到第二点-清晰地阐述动机和理由，更符合NLP技术当前发展的进度，虽然脱离第一点和第三点去做研究容易脱离实际问题，但是考虑地太全面反而难以推进技术的发展，我觉得可以根据某些大神总结出来的一些值得研究的点，结合NLP中的实际任务来跟进研究即可。</font>]]></content>
    
    
    
    <tags>
      
      <tag>bias</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bias in NLP &amp; Debiasing methods</title>
    <link href="/2021/06/10/Bias-in-NLP-Debiasing-methods/"/>
    <url>/2021/06/10/Bias-in-NLP-Debiasing-methods/</url>
    
    <content type="html"><![CDATA[<h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>主要关注社会偏见在NLP 的<strong>word embedding </strong>及 <strong>sentence embedding</strong> 中的体现，其中前者的研究出现较早，目的是探究各类词向量中存在的社会偏见（以性别偏见为主），并寻找一种去偏方法来消解或减弱这种社会偏见与刻板印象；而后者则是探究BERT等上下文编码器出现后，NLP下游任务中更多地采用了这种句子级表示的编码方法，这种方法可能依旧存在的社会偏见，并寻找方法取消减这种社会偏见。</p><p><strong>社会偏见在NLP中的影响</strong>：以<strong>Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation</strong> 一文中提到的指代消解问题为例，“医生雇佣秘书是因为他的病人很多”，“医生雇佣秘书是因为她的病人很多”，两句话中的代词都应该指医生，但是由于词向量中存在医生大多为男性、秘书多为女性这种社会刻板印象，因此模型很容易将第二句话中的她认为是代指秘书。词向量中的性别偏见，主要是源于使用人类社会中存在这种社会偏见的语料训练所致，这些偏见被模型敏锐地捕捉到，然而这些偏见从客观角度来看，并不应该被下游任务所利用，这是将人类社会的陋习教给机器。</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200814104321564.png" alt="image-20200814104321564"></p><p>不同的任务中对于性别或其它社会偏见造成的影响不同，社会偏见除了来自于人类社会中固有的偏见被机器敏锐地捕捉到设置扩大化，还有一个原因是数据集中男行相关的语料较多、女性相关语料较少的数据分布不均问题。</p><h2 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h2><ul><li>词embedding 或句子 embedding 事前或事后去偏；</li><li>探究这些社会偏见在各类 NLP 任务中的影响，并对其评估；</li><li>针对具体任务进行偏见消减；</li></ul><p><strong>趋势</strong>：第一点提出较早，但是进展并不大，从ACL 2020 唯有的两篇去偏的文章提出的算法来看，这一点的研究还在对16年提出的 Hard Debias 进行扩展，18、19年的关于去偏的文章也主要关注在事前去偏以及事后去偏的一些改进；第二、三点的研究热度要比第一点稍高（ACL 2020 有六七篇论文是讨论这些内容的），主要就是探究 bias 在embedding中的影响、对下游任务的影响，以及如何取评估这种影响；针对具体任务的研究，机器翻译领域稍多，其它领域较少；</p><h2 id="评测数据及方法"><a href="#评测数据及方法" class="headerlink" title="评测数据及方法"></a>评测数据及方法</h2><h3 id="WEAT"><a href="#WEAT" class="headerlink" title="WEAT"></a>WEAT</h3><p><a href="https://arxiv.org/abs/1608.07187">Semantics derived automatically from language corpora contain human-like biases （2016）</a>一文中提出的 <strong>Word Embedding Association Test</strong> 方法。</p><p>通过置换检验的方法来证明和量化偏差：考虑两个目标词集合（例如，<code>程序员、工程师、科学家……</code> 和 <code>护士、老师、图书管理员……</code>）以及两个属性词集合（例如，<code>man、male……</code> 和 <code>woman、female……</code>）零假设是指两组目标词与两组属性词的相对相似度没有差异。置换检测 通过计算随机置换一个属性词会产生可观察到的样本均值差异 的可能性 来测量零假设的似然率。</p><p>令 X 和 Y 分别代表相同大小的两个目标词集合，A、B 则为属性词集合，令 $cos(\vec a,\vec b)$ 代表两个向量之间的角度。检测统计量为：</p><script type="math/tex; mode=display">s(X,Y,A,B)=\sum_{x\in X}s(x,A,B)-\sum_{y\in Y}s(y,A,B) \\where \quad s(w,A,B)=mean_{a\in A}cos(\vec w,\vec a)-mean_{b\in B}cos(\vec w,\vec b)</script><p>换句话说，$s(w,A,B)$测量了词 w 和属性之间的关联性，$s(X,Y,A,B)$ 测量了两个目标词集合与属性词之间的关联度差异；</p><p>令 $\{(X_i,Y_i)\}_i$ 代表 $X \cup Y$ 分成两个带线啊哦相等的集合的所有划分，置换检测的单方面 p 值为：</p><script type="math/tex; mode=display">Pr_i[s(X_i,Y_i,A,B)>s(X,Y,A,B)]</script><p>效应量为：</p><script type="math/tex; mode=display">\frac{mean_{x\in X}s(x,A,B)-mean_{y\in Y}s(y,A,B)}{std-dev_{w\in X\cup Y}s(w,A,B)}</script><p>它是对两个分布(目标和属性之间的关联)的分离程度的规范化度量，更大的效应量反映了更严重的偏差。</p><h3 id="SEAT"><a href="#SEAT" class="headerlink" title="SEAT"></a>SEAT</h3><p><a href="https://arxiv.org/pdf/1903.10561.pdf">On Measuring Social Biases in Sentence Encoders (2019)</a> ~这篇文章的附录有很详细的关于一些测试的说明，以及大量的例子，可以帮助理解bias的问题~一文中提出的与<strong>WEAT</strong> 类似的方法，用于句子 embedding 的偏差检测。</p><p>为了将一个词级别的检测扩展到句子上下文中，<strong>SEAT</strong> 将一个词放入几个缺乏语义(semantically bleached) 的句子模板中，例如<code>This is &lt;word&gt;.</code>, <code>&lt;word&gt; is here.</code>, <code>This will &lt;word&gt;.</code>等，这些模板大量使用指示语，除了插入其中的 词项 外，没有什么特别的意思。例如：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200814220934848.png" alt="image-20200814220934848"></p><h3 id="其它的一些检测方法及数据集"><a href="#其它的一些检测方法及数据集" class="headerlink" title="其它的一些检测方法及数据集"></a>其它的一些检测方法及数据集</h3><ul><li><strong>Equity Evaluation Corpus (EEC):</strong> 用于检测情绪分析模型中的某些种族和性别偏见的语料库；<a href="https://arxiv.org/abs/1805.04508；">https://arxiv.org/abs/1805.04508；</a></li><li><strong>WinoBias：</strong> 用于测试指代消解中存在的性别偏见问题的基准数据集；<a href="https://www.aclweb.org/anthology/N18-2003.pdf；">https://www.aclweb.org/anthology/N18-2003.pdf；</a></li><li><strong>WiKiGenderBias</strong>：用于评测关系抽取任务中的性别偏见问题；</li><li><strong>WinoMT</strong>：机器翻译中的性别偏见评估数据集；</li></ul><h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="Hard-Debias"><a href="#Hard-Debias" class="headerlink" title="Hard Debias"></a>Hard Debias</h3><p><a href="https://arxiv.org/pdf/1607.06520.pdf">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings（2016）</a> 一文中提出的词向量<strong>事后去偏</strong>方法：</p><ol><li><p>首先定义子空间 <script type="math/tex">B=\{b_1,\dots,b_k\}\subset \mathbb{R}^d</script> 其中的向量彼此正交，当 k=1 时，这个子空间就时简单代表一个方向，向量 v 在子空间 B上的投影的定义为：$v_B=\sum_{j=1}^k(v\cdot b_j)b_j$ ; 之后定义一个集合 $D_1,D_2,\dots,D_n\subset W$, W 为所有词的集合，$D_i$ 是{girl, boy}, {man, woman}形式的词对，令$\mu_i := \sum_{w\in D_i}\vec w / |D_i|$，对这些平均向量集 $C:=\sum_{i=1}^n \sum_{w\in D_i}(\vec w -\mu_i)^T(\vec w-\mu_i).|D_i|$ 进行奇异值分解 SVD(C) ，取前 k 行作为 bias 子空间B。</p></li><li><p>额外定义一些需要中和处理的词表（性别中性词语，例如“医生”、“教师”等一些不应该特定于性别的词语）$N\subseteq W$ 和 均衡集合族 $\varepsilon =\{E_1,E_2,\dots,E_m\}$，对于每个词 $w\in N$，对其 re-embedded得到 $\vec w:=\vec w-\vec w_B)/||\vec w-\vec w_B$。</p><p>对于$E\in \varepsilon$ ,令：</p><script type="math/tex; mode=display">\mu:=\sum_{w\in E }w/|E|</script><script type="math/tex; mode=display">v:=\mu-\mu_B</script><script type="math/tex; mode=display">For\ each\ w\in E,\ \vec w:=v+\sqrt{1-||v||^2}\frac{\vec w_B-\mu_B}{||\vec w_B-\mu_B||}</script></li></ol><h3 id="GN-Glove-Gender-Neutral"><a href="#GN-Glove-Gender-Neutral" class="headerlink" title="GN-Glove (Gender-Neutral)"></a>GN-Glove (Gender-Neutral)</h3><p>论文链接：<a href="https://arxiv.org/abs/1809.01496">https://arxiv.org/abs/1809.01496</a> EMNLP 2018</p><p>大致思想是根据Glove 的思想，结合<strong>性别中立</strong>的理念重新设计训练了 Glove，在某些维度上消除了性别信息，中和其它维度，并属于<strong>事前去偏方法</strong>；具体介绍可参考一篇知乎博客（GN-Glove 性别中立的词嵌入学习 - 论智的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/44794638）">https://zhuanlan.zhihu.com/p/44794638）</a></p><h3 id="GP-Glove-Gender-preserving"><a href="#GP-Glove-Gender-preserving" class="headerlink" title="GP-Glove (Gender-preserving)"></a>GP-Glove (Gender-preserving)</h3><p>论文链接：<a href="https://arxiv.org/pdf/1906.00742.pdf">https://arxiv.org/pdf/1906.00742.pdf</a> ACL 2019</p><p>GP-Glove 保留了无歧视的性别信息，同时去除了刻板的歧视性性别偏见。文中考虑了四种信息：<em>feminine</em>、<em>masculine</em>、<em>gender-neutral</em>、<em>stereotypical</em>，（举一些例子，前两类：女服务员、家庭主妇、congressman、businessman；gender-neutral：作曲家、社会名流、预备演员；stereotype：保姆、housekeeper、colonel（陆军上校）、captain等）代表了性别于偏见之间的关系，并提出了一种去偏方法：（a）保留 <em>feminine</em> 与 <em>masculine</em> 词语中的性别相关的信息；（b）保留 <em>gender-neutral</em> 词的中立性；（c）去除 <em>stereotypical</em> 词语的偏见。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>通过将数据集中关于性别的实体替换为表示相反性别的单词，然后对原始数据和被交换数据的并集同时训练。（WinoBias那篇文章提出的一个简单方法）</p><h3 id="对抗"><a href="#对抗" class="headerlink" title="对抗"></a>对抗</h3><p><a href="https://arxiv.org/pdf/1808.06640.pdf">Adversarial Removal of Demographic Attributes from Text Data</a> 一文中探讨了使用对抗训练来移除encoder中包含的人口统计信息的效果，作者发现对抗训练并不能比直接训练的post-hoc 分类器效果好。并得出了一个结论：不要依赖对抗训练来实现对敏感特征的不变表示。</p><blockquote><p>Do not rely on the adversarial training to achieve invariant representation to sensitive features.</p></blockquote><p>这类方法中，首先对输入进行编码，然后训练两个分类器，一个目标预测器使用编码的输入来预测目标NLP任务，另一个 protected-attribute 预测器，利用编码后的输入来预测保护的属性，两个分类器联合学习，使目标任务预测器准确率最大，而后者准确率最小化。</p><h2 id="ACL-2020-上的几篇文章"><a href="#ACL-2020-上的几篇文章" class="headerlink" title="ACL 2020 上的几篇文章"></a>ACL 2020 上的几篇文章</h2><h3 id="句向量事后去偏"><a href="#句向量事后去偏" class="headerlink" title="句向量事后去偏"></a>句向量事后去偏</h3><p><a href="https://w-lw.github.io/2020/09/18/%E5%8F%A5%E5%AD%90%E5%8E%BB%E5%81%8F/">https://w-lw.github.io/2020/09/18/%E5%8F%A5%E5%AD%90%E5%8E%BB%E5%81%8F/</a></p><h3 id="词向量事后去偏"><a href="#词向量事后去偏" class="headerlink" title="词向量事后去偏"></a>词向量事后去偏</h3><p>Link: <a href="https://arxiv.org/abs/2005.00965">Double-Hard Debias: TailoringWord Embeddings for Gender Bias Mitigation</a></p><p>本文关注的是词向量的性别去偏，依旧采用的是与 <strong>Hard Debias</strong> 方法类似的 事后(post-hoc) 去偏。</p><p>文中提出的 <strong>Double-Hard Debias</strong> 方法是针对 <strong>Hard Debias</strong> 方法未能考虑到词频对于词向量的影响——词频会扭曲性别偏置子空间的方向，使得<strong>Hard Debias</strong> 方法效果有限。<strong>Double-Hard Debias</strong> 包括两个步骤：首先，通过减去词频相关的分量，将词向量投影到一个过渡子空间中，减轻词频对于性别子空间的影响；之后再使用<strong>Hard Debias</strong> 方法减轻性别偏见。</p><p>下图说明了，当词频改变之后，不同的 bias pair 之间的余弦相似度会受到影响，这也证明了词频在性别去偏中有不可忽略的影响。</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813183937213.png" alt="image-20200813183937213"></p><p><strong>方法</strong>：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813205102245.png" alt="image-20200813205102245"></p><p><strong>2</strong>：这一步是计算所有embedding 的平均；（里面的 V 就是 W，这里应该是写错了）</p><p><strong>3：</strong> 这一步对去中心化后的所有词向量做PCA，得到 d 个 u 向量（为何取d 个不太清楚，引文中也是取d），</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813205653825.png" alt="image-20200813205653825"></p><p><strong>5~12：</strong>：通过遍历每个 ui ，计算出去掉该 ui 后的去偏效果；</p><p><strong>14-16：</strong> 去掉去偏效果最好的那个 ui；（引文中是设置了一个参数 D，去掉前 D 个 ui 向量，这篇文章则是通过遍历找到影响最大的那个）</p><p><strong>18：</strong> <strong>Hard Debias</strong>；</p><p><strong>实验</strong></p><p><strong>指代消解实验</strong></p><p>WinoBias数据集</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813215433586.png" alt="image-20200813215433586"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813215524018.png" alt="image-20200813215524018"></p><p>在 WinoBias 数据集上的结果如下：</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813215224560.png" alt="image-20200813215224560"></p><p>Diff-1 越小代表模型受到性别偏差的影响越小。</p><p><strong>The Word Embeddings Association Test(WEAT)</strong></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813221051109.png" alt="image-20200813221051109"></p><p><strong>Neighborhood Metric</strong></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813221051109.png" alt="image-20200813221224781"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200813221828723.png" alt="image-20200813221828723"></p><h3 id="关系抽取中的性别偏见"><a href="#关系抽取中的性别偏见" class="headerlink" title="关系抽取中的性别偏见"></a>关系抽取中的性别偏见</h3><p>Link: <a href="https://www.aclweb.org/anthology/2020.acl-main.265/">Towards Understanding Gender Bias in Relation Extraction</a> </p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>现有的（Neural Relation Extraction）NRE系统主要关注在提高准确性上，却没有评估NER系统中存在的社会偏见。本文提出了一个数据集 <strong>WiKiGenderBias</strong>，由超过45,000 个句子（包含10%的人工标注的测试集），用于分析关系抽取系统中的性别偏见。作者发现，在提取某些上位关系（例如职业）时，NRE系统在目标实体的性别不同是，表现也存在差异，然而在提取诸如生日、出生此等关系时，这种差异并不会出现。作者还分析了现有的偏差消解技术，例如姓名隐匿化、词嵌入去偏和数据增强，在报错测试性能和减少偏差方面对NRE系统的影响。不幸的是，由于NRE模型严重依赖 surface level cues，作者发现现有的偏差消解技术对NRE由负面影响，作者的分析为未来量化和减轻NRE偏差奠定了基础。</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200815200837708.png" alt="image-20200815200837708"></p><p>大致看了看文中的图表，这篇文章主要就是提出关系抽取中也存在性别bias 的问题， 文中提出了一个可用于测试这个问题的数据集，并用了一些常用的方法（平衡两种性别的训练数据量、数据增强、词向量去偏）进行去偏实验，实验效果并不理想，大部分技术都会导致模型性能下降，而且没有一种技术能够有效缩小性别之间的性能差距。</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200815201622027.png" alt="image-20200815201622027"></p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200815202048063.png" alt="image-20200815202048063"></p><h3 id="Social-Bias-Frames-Reasoning-about-Social-and-Power-Implications-of-Language"><a href="#Social-Bias-Frames-Reasoning-about-Social-and-Power-Implications-of-Language" class="headerlink" title="Social Bias Frames: Reasoning about Social and Power Implications of Language"></a>Social Bias Frames: Reasoning about Social and Power Implications of Language</h3><p>LINK：<a href="https://www.aclweb.org/anthology/2020.acl-main.486/">Social Bias Frames: Reasoning about Social and Power Implications of Language</a></p><h3 id="机器翻译中的性别偏见"><a href="#机器翻译中的性别偏见" class="headerlink" title="机器翻译中的性别偏见"></a>机器翻译中的性别偏见</h3><h4 id="Link：Reducing-Gender-Bias-in-Neural-Machine-Translation-as-a-Domain-Adaptation-Problem"><a href="#Link：Reducing-Gender-Bias-in-Neural-Machine-Translation-as-a-Domain-Adaptation-Problem" class="headerlink" title="Link：Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem"></a>Link：<a href="https://www.aclweb.org/anthology/2020.acl-main.690/">Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem</a></h4><h4 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h4><p>由于涉及女性的句子通常比涉及男性的句子要少，在神经机器翻译中，性别偏见已经被证明会降低翻译质量，特别是当目的语言有语法上的性别时。最近的 WinoMT 挑战集可以直接用于测试这些问题的影响。理想情况下，我们可以通过简单地在训练前消除所有数据的偏见来减少偏见，但是这本身就很有挑战性。本文不是试图创建一个平衡的数据集，而是在一个规模较小的可信任的性别平衡的数据集上使用迁移学习。这种方法在性别去偏问题上提供了强大且一致的提升。</p><p>用的数据集是下面那篇文章中提出来的WinoMT。</p><h4 id="Link-Evaluating-Gender-Bias-in-Machine-Translation"><a href="#Link-Evaluating-Gender-Bias-in-Machine-Translation" class="headerlink" title="Link: Evaluating Gender Bias in Machine Translation"></a>Link: <a href="https://www.aclweb.org/anthology/P19-1164/">Evaluating Gender Bias in Machine Translation</a></h4><p>这篇文章则是提出了一个用于分析机器翻译中的性别偏见问题的挑战集 <strong>WinoMT</strong>和评估协议。</p><h4 id="Link：Gender-Bias-in-Multilingual-Embeddings-and-Cross-Lingual-Transfer"><a href="#Link：Gender-Bias-in-Multilingual-Embeddings-and-Cross-Lingual-Transfer" class="headerlink" title="Link：Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer"></a>Link：<a href="https://www.aclweb.org/anthology/2020.acl-main.260.pdf">Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer</a></h4><p>这篇文章则是研究了多语言嵌入中的性别偏见，以及它是如何影响到NLP应用中的迁移学习的。作者为偏差分析创建了一个多语言数据集，并从内在和外在两个方面提出了集中量化多语言表示偏差的方法。</p><h3 id="通过后验正则化的方法减小性别偏见在分布中的扩大"><a href="#通过后验正则化的方法减小性别偏见在分布中的扩大" class="headerlink" title="通过后验正则化的方法减小性别偏见在分布中的扩大"></a>通过后验正则化的方法减小性别偏见在分布中的扩大</h3><p>Link：<a href="https://www.aclweb.org/anthology/2020.acl-main.264/">Mitigating Gender Bias Amplification in Distribution by Posterior Regularization</a></p><p>摘要：一些研究表明，机器学习技术无意中捕获了隐藏在语料库中的社会偏见，并进一步放大了它。然而，这些分析只是基于模型顶层的预测而得到的。本文作者从分布的角度研究了性别偏见放大问题，并证明偏见在预测的标签概率分布上被放大了。作者进一步提出了一种基于厚颜正则化的偏差消解方法，在性能损失很小的情况下，该方法几乎可以消除分布中的偏见放大。该研究为理解偏见放大提供了线索。</p><p><img src="https://raw.githubusercontent.com/W-lw/imgBed/master/imgs/image-20200815213909263.png" alt="image-20200815213909263"></p><h2 id="一些相关的文章"><a href="#一些相关的文章" class="headerlink" title="一些相关的文章"></a>一些相关的文章</h2><h3 id="Nurse-is-Closer-to-Woman-than-Surgeon-Mitigating-Gender-Biased-Proximities-in-Word-Embeddings"><a href="#Nurse-is-Closer-to-Woman-than-Surgeon-Mitigating-Gender-Biased-Proximities-in-Word-Embeddings" class="headerlink" title="Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings"></a><a href="https://arxiv.org/abs/2006.01938">Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings</a></h3><p>来源：TACL 2020</p><p>文章题目风格与经典的 Hard Debias 那篇文章类似；</p><p><strong>摘要</strong>：作者认为现有的词嵌入后处理的去偏方法无法消除隐藏在词向量空间位置中的性别偏见。文中提出了一种 <strong>RAN-Debias</strong> 方法，它不仅消除了词向量中存在的偏见，还改变了相邻向量的空间分布，在保持最小语义偏移的同时实现了 bias-free 的设定。此外，文中还提出了一种新的偏见评估方法-<strong>Gender-based Illicit Proximity Estimate (GIPE)</strong>，它能衡量性别偏见所导致的词向量不适当接近的程度。</p><h3 id="Black-is-to-Criminal-as-Caucasian-is-to-Police-Detecting-and-Removing-Multiclass-Bias-in-Word-Embeddings"><a href="#Black-is-to-Criminal-as-Caucasian-is-to-Police-Detecting-and-Removing-Multiclass-Bias-in-Word-Embeddings" class="headerlink" title="Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings"></a><a href="https://arxiv.org/abs/1904.04047">Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings</a></h3><p>来源：NAACL2019</p><p>标题风格同上</p><p>这篇文章就是针对人种、宗教等多类别设置的偏见，是对Hard Debias 一文工作的扩展。文中提出了一种新的评估多类去偏的方法。（这个方法在之前ACL 2020的那篇句子去偏的文章种用到了）</p><h3 id="Assessing-Demographic-Bias-in-Named-Entity-Recognition"><a href="#Assessing-Demographic-Bias-in-Named-Entity-Recognition" class="headerlink" title="Assessing Demographic Bias in Named Entity Recognition"></a><a href="https://arxiv.org/abs/2008.03415">Assessing Demographic Bias in Named Entity Recognition</a></h3><p>来源：AKBC workshop 2020</p><p>探究了NER种关于人口统计偏见的问题；</p><h3 id="Causal-Mediation-Analysis-for-Interpreting-Neural-NLP-The-Case-of-Gender-Bias"><a href="#Causal-Mediation-Analysis-for-Interpreting-Neural-NLP-The-Case-of-Gender-Bias" class="headerlink" title="Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias"></a><a href="https://arxiv.org/abs/2004.12265">Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias</a></h3><p>来源：arxiv 2004</p><p>从因果中介分析理论的角度来解释神经模型，并将该方法用在预训练的 Transformer 语言模型上，对性别偏见进行分析；</p><h3 id="…"><a href="#…" class="headerlink" title="…"></a>…</h3>]]></content>
    
    
    
    <tags>
      
      <tag>bias</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/06/08/hello-world/"/>
    <url>/2021/06/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
